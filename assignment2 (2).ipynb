{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIADS 543 Assignment 2: Clustering\n",
    "\n",
    "In this week's assignment you'll gain experience applying different clustering methods, computing cluster labels, and selecting an optimal number of clusters based on quality assessment metrics. In addition, you'll gain experience working with working with text data. \n",
    "\n",
    "We'll continue to use the \"beer\" dataset, but this time focused more on the *text* of beer reviews. In the \"bag of words\" scenario, each review is represented by a sparse vector that's filled in according to which specific terms are found in that review. (As a reminder, we generally use \"term\" to refer to a word or phrase that's part of a text representation - since as you'll see below, when working with text it's very useful to work with phrases as features as well as individual words.) In particular, we'll treat single terms and two-term bigrams (phrases) as features here. You'll apply a simple Vectorizer to process the text, which will also be a useful prelude to next week's upcoming assignment on topic models. To familiarize yourself with how a text passage can be represented as a vector of word counts or weights, the \"bag of words\" model, *please read Sec. 7.3-7.5 in the textbook*. \n",
    "\n",
    "As usual, please read through the entire assignment before starting, to get an idea of how the questions relate to each other and the overall goals.\n",
    "\n",
    "*Please note that for autograder messages that check a list, it will report any problems using a list index starting at zero, i.e. the first list element is called \"element 0\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import some necessary libararies \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(precision = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.  K-means clustering on text documents (25 points)\n",
    "\n",
    "In this question you'll apply k-means clustering to a set of text documents to gain some insight into the nature of the content. We're going to use the same beer dataset as last week, but this time we'll focus on the 'text' column that contains written reviews of the tasty beverage.\n",
    "\n",
    "This scenario uses the TfidfVectorizer class (described in Sec. 7.5), to convert each textual review to a numeric vector. We're going to cover text processing in more detail next week, including TfidfVectorizer and its many parameters, but for now we've provided you the code that reads in the dataset, and converts each text review into a sparse numeric vector. \n",
    "\n",
    "A general rule when clustering text documents is that *noun phrases* work well as features. For now, we'll ignore parts of speech, but we will use both single words and *bigrams* (two-word phrases) as features. This is easy to do with the Vectorizer by setting the ngram_range property to (1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# This function returns:\n",
    "# - a matrix X with one row per document (review). Each row is a sparse\n",
    "# vector containing tf.idf term weights for the words in the document.\n",
    "#\n",
    "# - the vectorizor used to create X\n",
    "# \n",
    "# - the actual reviews used as input to the vectorizer\n",
    "\n",
    "def get_beer_reviews_vectorized(top_n = -1, ngram_range = (1,1), max_features = 1000):\n",
    "    df = pd.read_csv('./assets/beer2.csv')['text']\n",
    "    df = df.dropna()   # drop any rows with empty reviews\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=max_features,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 ngram_range = ngram_range,\n",
    "                                 use_idf=True)\n",
    "    if (top_n >= 0):\n",
    "        review_instances = df.values[0:top_n]\n",
    "    else:\n",
    "        review_instances = df.values\n",
    "    \n",
    "    X = vectorizer.fit_transform(review_instances) \n",
    "    \n",
    "    return (X, vectorizer, review_instances)\n",
    "\n",
    "def print_cluster_features(vectorizer, centroids, n_clusters, top_n_features):\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(n_clusters):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in centroids[i, :top_n_features]:\n",
    "            print(' [%s]' % terms[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big problem in applying $K$-means (and many other unsupervised clustering-type methods) is that we need to specify the number of clusters $K$ in advance - but we don't know in advance what value of $K$ will give the best-quality clustering!  So, to pick the \"best\" number of clusters we're going to rely on calculating some automated measures of cluster quality. You'll compute these cluster quality measures for each choice of $K$, and then pick the value of $K$ that gives the best value of the quality measure(s).\n",
    "\n",
    "We seen a few ways to assess cluster quality, including the silhouette score, but we're going to try out two alternative cluster quality measures that you should be aware of. They reward different ideas of \"quality\" for a clustering, but like the silhouette score, do not require ground-truth labels to compute quality (which is good, because we don't have them). These two measures are:\n",
    "\n",
    "The Davies-Bouldin score https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html \n",
    "*Lower* is better for the Davies-Bouldin score.\n",
    "\n",
    "The Calinski-Harabasz index  https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index\n",
    "*Higher* is better for the Calinski-Harabasz index.\n",
    "\n",
    "We're looking for clusterings that have a *high* Calinski-Harabasz index, but a *low* Davies-Bouldin score.\n",
    "\n",
    "Okay - here's what you need to do for Question 1:\n",
    "\n",
    "1. Get the vectorized reviews using the get_beer_reviews_vectorized() function as shown below. For speed reasons we will only analyze a sample of 5000 reviews and not the whole set. The vectorized reviews will be output to the matrix X, which should have 5000 rows (one per review) and 1000 columns (one per word/term in the vocabulary produced by the vectorizer).\n",
    "\n",
    "2. For each choice of K from 2 to 9, run k-means clustering on these reviews. For each K, compute the above two cluster quality scores on the resulting clustering. IMPORTANT: You won't be including the code to run the whole loop in your autograded code submission: you're just trying to find the optimal value for $K$ for now.\n",
    "\n",
    "IMPORTANT: *when running KMeans, set init='k-means++', max_iter=100, n_init=1, and random_state=42.*\n",
    "\n",
    "After you perform your clustering run in Step 2, you should see there's one clearly superior choice for K that gives the best value for both metrics (high Calinski-Harabasz score, low Davies-Bouldin score). \n",
    "\n",
    "3. Now you will write your assignment code that does a single clustering using that optimal K, and finds what the predominant terms are in the resulting clusters.  Remember, each cluster is a collection of vectors, each of which represents a beer review. To find good representative terms for each cluster, you want to know what the \"typical\" review looks like for each of the K clusters. A \"typical\" review for a cluster is just the mean of all the vectors (reviews) that belong to that cluster. This vector is known as the cluster centroid or center. *Note that when you run K-Means you don't have to compute the cluster centers yourself -- they are computed for you by K-Means after you run 'fit'. Just use the resulting cluster_centers_ property.*\n",
    "\n",
    "4. The cluster center, since it's a just the mean of a bunch of review vectors, is itself just a vector of the same dimension as the input review vectors: think of the cluster center vector like a \"word cloud\" that holds the weight of each possible word in a review. So to get the most predominant terms in a vector, just sort the entries of the cluster centroid *from highest to lowest term weight*, and return the corresponding term strings. *You can do this sorting with one line of code, on all the cluster centers simultaneously, by applying argsort(..) to the cluster_centers_ array.*\n",
    "\n",
    "Take a look at the list of top-weighted terms that come out for each cluster: do they look reasonable? Are the terms in each cluster similar in some way to each other? (We will revisit these terms in the next question.)  Remember that you can get the term strings that correspond to the review vector entries by using the vectorizer's get_feature_names() method.  *You can use the `print_cluster_features` example code above for reference on how to do this.*  \n",
    "\n",
    "Your function should return a list that has $K$ elements, where $K$ is the optimal number of clusters you found above. Each of these $K$ elements should itself contain a list of the top 10 terms (strings) for that cluster. These terms should be sorted by highest to lowest term weight value as stored in the cluster centroid.\n",
    "\n",
    "After you submit your solution using the parameters we supplied, we encourage you to play with different values for the parameters of TfidfVectorizer, to see how the results change, because the results of this kind of clustering can be very sensitive to those parameters, which control how the text is processed, what defines what a \"term\" is, and which words are kept as features in the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the following code to preprocess the dataset. For efficiency reasons we're going to sample a subset of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For performance reasons we only use the first 5000 reviews in this question\n",
    "(X, vectorizer, review_instances) = get_beer_reviews_vectorized(5000, (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K value: 2\n",
      "Calinski: 66.36912593606624\n",
      "Davies: 7.103724957438543\n",
      "K value: 3\n",
      "Calinski: 51.20269894745629\n",
      "Davies: 9.35104391077595\n",
      "K value: 4\n",
      "Calinski: 40.61312339863478\n",
      "Davies: 9.592438827030367\n",
      "K value: 5\n",
      "Calinski: 42.95571301601\n",
      "Davies: 8.197282591441107\n",
      "K value: 6\n",
      "Calinski: 38.14493770875936\n",
      "Davies: 8.287681685515775\n",
      "K value: 7\n",
      "Calinski: 35.09722200514753\n",
      "Davies: 7.976206385944392\n",
      "K value: 8\n",
      "Calinski: 31.834934126437304\n",
      "Davies: 7.955940974835475\n",
      "K value: 9\n",
      "Calinski: 29.53090074292199\n",
      "Davies: 7.743336777219178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "for i in range(2,10):\n",
    "    kmeans = KMeans(init=\"k-means++\", max_iter=100, n_init=1, n_clusters=i, random_state=42).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    calinski = metrics.calinski_harabasz_score(X.toarray(), labels)\n",
    "    davies = metrics.davies_bouldin_score(X.toarray(), labels)\n",
    "    print(\"K value:\", i)\n",
    "    print(\"Calinski:\", calinski)\n",
    "    print(\"Davies:\", davies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.947e-03, 7.001e-03, 2.971e-03, ..., 4.293e-04, 0.000e+00,\n",
       "        5.621e-05],\n",
       "       [4.655e-03, 6.402e-03, 3.254e-03, ..., 1.364e-02, 4.012e-03,\n",
       "        3.327e-03]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(init=\"k-means++\", max_iter=100, n_init=1, n_clusters=2, random_state=42).fit(X)\n",
    "result = kmeans.cluster_centers_\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: [roasted coffee] [chocolate coffee] [coffee chocolate] [pitch black] [ris] [bitter chocolate] [milk chocolate] [dark chocolate] [black color] [pitch]\n",
      "Cluster 1: [hefe] [apa] [hazy golden] [pale malt] [gold color] [adjunct] [orange color] [ginger] [golden amber] [orange amber]\n"
     ]
    }
   ],
   "source": [
    "#def print_cluster_features(vectorizer, centroids, n_clusters, top_n_features):\n",
    "result_sorted = np.argsort(result)[::-1]\n",
    "#for x in result_sorted:\n",
    "#    result = result[::-1]\n",
    "#for x in result:\n",
    "#    x.sort()\n",
    "\n",
    "print_cluster_features(vectorizer, result_sorted, 2, 10)\n",
    "\n",
    "#result_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[778 161 184 ... 397 474 629]\n",
      " [444  37 426 ... 225 183 160]]\n"
     ]
    }
   ],
   "source": [
    "print(result_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.947e-03 7.001e-03 2.971e-03 7.150e-03 5.551e-03 4.711e-03 3.716e-03\n",
      " 4.345e-03 3.854e-03 5.085e-03 3.482e-03 4.581e-03 1.843e-03 2.974e-03\n",
      " 4.087e-03 1.300e-02 2.262e-03 6.389e-03 2.835e-03 2.292e-03 3.086e-03\n",
      " 0.000e+00 1.102e-02 3.622e-03 5.615e-03 2.722e-03 3.813e-03 2.272e-03\n",
      " 3.317e-02 3.010e-03 1.011e-02 2.432e-03 7.052e-03 3.511e-03 1.087e-03\n",
      " 4.321e-03 1.730e-03 0.000e+00 4.002e-03 1.467e-02 1.976e-03 5.575e-03\n",
      " 1.968e-03 7.782e-04 0.000e+00 2.871e-02 2.951e-03 3.076e-03 8.802e-03\n",
      " 3.575e-03 2.396e-03 1.700e-03 7.390e-03 6.861e-03 4.535e-03 2.064e-03\n",
      " 5.714e-03 7.417e-03 7.674e-03 1.021e-02 2.342e-03 1.204e-03 3.162e-03\n",
      " 4.754e-03 3.390e-04 5.138e-03 2.375e-03 2.303e-03 5.268e-03 2.306e-03\n",
      " 1.196e-03 1.474e-03 2.546e-03 2.059e-03 4.637e-03 3.767e-03 8.174e-03\n",
      " 1.697e-03 6.173e-03 5.024e-03 4.883e-03 1.257e-03 2.019e-03 8.974e-03\n",
      " 9.970e-03 1.874e-02 1.260e-03 5.979e-04 3.118e-02 2.941e-03 2.937e-02\n",
      " 1.254e-02 3.949e-03 1.350e-03 2.029e-02 2.415e-03 6.758e-03 6.473e-02\n",
      " 1.370e-02 6.829e-04 3.560e-03 2.208e-03 2.163e-02 2.533e-02 2.885e-03\n",
      " 3.693e-03 7.845e-03 4.866e-03 5.131e-03 2.390e-02 1.767e-03 5.356e-03\n",
      " 3.036e-03 2.411e-03 2.900e-03 2.012e-02 5.571e-03 3.285e-03 4.660e-04\n",
      " 1.553e-04 1.646e-02 3.701e-03 4.873e-03 2.879e-03 2.274e-03 1.304e-03\n",
      " 2.993e-03 5.967e-02 5.780e-03 2.531e-03 1.360e-02 1.678e-02 1.119e-02\n",
      " 3.129e-03 6.222e-03 3.527e-03 4.679e-03 2.235e-02 1.498e-03 3.608e-03\n",
      " 2.024e-03 1.993e-03 6.685e-03 2.582e-02 2.242e-03 2.572e-03 3.915e-03\n",
      " 7.945e-03 2.693e-02 2.732e-03 2.371e-03 1.478e-03 5.149e-03 1.586e-03\n",
      " 8.379e-04 5.383e-03 9.978e-03 5.312e-03 1.035e-02 5.726e-03 1.119e-01\n",
      " 1.738e-02 7.775e-03 1.654e-03 1.961e-03 9.636e-04 4.901e-03 1.306e-03\n",
      " 1.065e-03 1.710e-03 9.827e-04 6.220e-03 3.244e-03 0.000e+00 1.380e-04\n",
      " 0.000e+00 4.645e-03 1.557e-03 1.491e-03 1.120e-03 2.510e-03 4.249e-04\n",
      " 1.964e-02 1.106e-01 1.362e-02 5.463e-03 6.013e-03 1.985e-02 1.985e-03\n",
      " 1.909e-03 1.615e-03 2.052e-03 8.120e-04 1.125e-02 8.526e-03 3.299e-03\n",
      " 3.238e-03 9.538e-03 1.149e-02 6.354e-03 1.921e-03 4.518e-03 1.441e-02\n",
      " 5.252e-03 2.955e-03 2.282e-03 3.283e-03 1.676e-03 1.063e-04 4.749e-04\n",
      " 2.864e-04 1.850e-03 5.453e-03 3.513e-03 1.600e-03 6.522e-03 3.069e-03\n",
      " 2.984e-02 2.432e-03 7.657e-04 3.928e-03 7.243e-04 1.004e-04 1.289e-03\n",
      " 4.029e-03 9.579e-02 1.291e-03 3.367e-02 2.506e-02 1.610e-02 1.442e-02\n",
      " 3.118e-03 3.417e-03 2.811e-03 3.016e-03 1.378e-02 3.726e-03 3.779e-03\n",
      " 1.845e-02 8.839e-04 8.030e-03 2.520e-03 1.304e-02 8.110e-03 4.378e-03\n",
      " 1.385e-03 2.745e-03 2.390e-03 5.498e-03 6.569e-03 7.053e-03 3.163e-04\n",
      " 2.043e-03 3.661e-03 2.498e-03 6.427e-03 9.182e-03 3.044e-03 3.611e-03\n",
      " 3.153e-03 3.519e-03 7.871e-03 1.790e-03 1.861e-03 2.659e-03 2.468e-03\n",
      " 4.506e-03 1.797e-02 1.652e-02 4.699e-03 1.660e-02 3.077e-03 3.300e-03\n",
      " 8.713e-03 2.267e-03 1.850e-02 4.821e-03 2.109e-03 3.586e-03 1.439e-03\n",
      " 2.994e-03 1.384e-03 9.599e-03 2.068e-03 4.661e-03 1.103e-02 5.990e-03\n",
      " 2.929e-03 4.303e-03 1.050e-02 1.192e-02 1.788e-03 4.810e-03 5.628e-03\n",
      " 5.632e-03 5.456e-03 4.307e-03 1.429e-02 2.891e-03 2.646e-03 3.935e-03\n",
      " 3.217e-03 4.011e-03 9.018e-03 5.758e-03 6.302e-03 4.753e-03 2.992e-03\n",
      " 3.089e-03 4.271e-03 2.211e-03 6.576e-03 1.004e-02 2.512e-03 1.117e-02\n",
      " 3.039e-03 1.808e-03 4.329e-03 4.173e-03 4.613e-03 2.577e-03 4.986e-03\n",
      " 1.357e-02 4.016e-03 4.278e-03 2.639e-03 4.990e-03 1.402e-03 2.912e-03\n",
      " 3.209e-03 7.243e-03 1.533e-02 4.544e-03 7.138e-04 6.954e-03 2.980e-02\n",
      " 2.929e-03 6.922e-03 2.574e-03 1.913e-03 2.850e-03 2.546e-03 3.539e-02\n",
      " 3.554e-03 3.697e-03 2.641e-02 2.858e-03 4.369e-03 2.067e-03 6.727e-04\n",
      " 3.928e-03 5.151e-04 1.091e-02 6.002e-03 1.518e-03 7.289e-03 3.507e-03\n",
      " 2.245e-03 2.538e-03 4.085e-03 5.780e-03 5.354e-04 2.512e-03 5.191e-03\n",
      " 1.683e-02 4.146e-03 1.570e-02 6.182e-03 1.547e-03 9.637e-04 1.404e-03\n",
      " 2.122e-03 2.999e-04 3.805e-03 4.700e-03 0.000e+00 4.095e-03 5.549e-03\n",
      " 2.264e-03 6.221e-03 2.674e-02 3.699e-03 2.269e-03 2.817e-03 2.546e-03\n",
      " 2.442e-03 3.163e-03 7.922e-03 9.506e-03 2.174e-04 0.000e+00 5.895e-04\n",
      " 0.000e+00 0.000e+00 1.350e-04 0.000e+00 2.047e-03 3.622e-02 4.255e-03\n",
      " 4.166e-03 3.393e-03 3.131e-03 2.437e-03 6.908e-03 4.178e-03 6.361e-03\n",
      " 1.708e-03 1.695e-03 1.131e-03 2.004e-03 1.506e-03 1.930e-03 1.178e-03\n",
      " 2.495e-02 5.143e-03 2.169e-03 8.122e-04 2.484e-03 2.318e-03 6.195e-03\n",
      " 1.528e-03 6.158e-03 2.216e-03 3.850e-03 9.970e-04 1.104e-03 0.000e+00\n",
      " 5.850e-03 3.642e-03 2.121e-03 3.579e-03 3.196e-03 4.077e-03 3.150e-03\n",
      " 3.787e-03 2.798e-03 3.840e-03 2.303e-03 4.300e-03 3.899e-03 2.306e-03\n",
      " 4.683e-04 3.384e-03 1.684e-02 0.000e+00 8.302e-03 7.530e-03 2.715e-03\n",
      " 6.228e-03 9.824e-03 3.052e-03 1.796e-03 3.212e-03 7.594e-03 5.157e-03\n",
      " 1.748e-02 2.180e-02 3.614e-03 5.796e-03 2.785e-03 1.130e-03 1.510e-03\n",
      " 1.056e-02 1.291e-03 7.335e-04 4.075e-03 1.309e-03 1.745e-03 1.970e-03\n",
      " 2.866e-03 1.607e-03 7.322e-04 2.596e-03 4.104e-03 1.853e-02 1.029e-03\n",
      " 3.980e-03 2.675e-03 3.180e-03 9.034e-03 2.751e-03 1.480e-02 4.094e-03\n",
      " 2.514e-03 4.028e-03 8.449e-03 2.344e-03 2.747e-03 3.127e-03 3.210e-03\n",
      " 2.045e-03 4.574e-03 8.282e-03 1.780e-03 1.447e-03 3.735e-04 3.267e-03\n",
      " 6.986e-03 2.417e-03 5.029e-04 8.114e-04 2.410e-02 2.329e-03 2.844e-03\n",
      " 2.766e-03 2.792e-03 2.517e-03 1.456e-03 1.790e-03 3.341e-03 7.286e-03\n",
      " 5.449e-03 4.332e-03 1.106e-02 8.199e-04 2.491e-02 4.259e-03 5.443e-03\n",
      " 2.634e-03 6.548e-03 3.438e-03 1.190e-03 1.576e-03 1.235e-03 2.520e-04\n",
      " 5.380e-03 3.734e-03 3.182e-03 5.100e-03 2.446e-03 1.467e-02 2.601e-03\n",
      " 7.558e-03 8.875e-03 3.327e-04 3.349e-04 3.797e-03 6.298e-03 1.286e-02\n",
      " 3.494e-02 2.207e-03 1.525e-03 8.491e-03 3.320e-03 3.159e-03 6.837e-03\n",
      " 6.051e-03 5.506e-03 3.830e-02 1.077e-03 2.713e-03 1.970e-03 2.704e-03\n",
      " 2.845e-03 7.092e-03 4.795e-03 3.981e-03 3.164e-02 5.644e-03 2.445e-03\n",
      " 1.188e-03 4.359e-03 2.012e-03 1.047e-02 4.688e-03 6.578e-03 8.959e-03\n",
      " 3.507e-03 3.281e-03 1.320e-02 1.288e-02 8.129e-03 1.185e-02 6.404e-03\n",
      " 7.793e-03 6.717e-03 8.228e-03 4.710e-03 3.767e-02 3.704e-03 1.268e-03\n",
      " 3.515e-03 3.624e-03 2.268e-03 3.609e-03 3.147e-02 1.272e-02 3.961e-03\n",
      " 0.000e+00 9.613e-03 2.657e-02 1.330e-02 9.765e-03 4.396e-03 8.021e-04\n",
      " 3.321e-03 2.284e-03 2.178e-03 5.750e-03 1.294e-02 3.148e-03 1.852e-02\n",
      " 1.181e-02 3.173e-03 5.639e-03 2.716e-03 5.656e-03 4.139e-03 1.495e-02\n",
      " 8.747e-03 3.560e-03 2.235e-03 1.890e-02 3.187e-03 1.220e-02 5.481e-03\n",
      " 2.584e-02 2.621e-03 1.118e-03 5.172e-03 1.791e-03 3.106e-03 9.342e-04\n",
      " 1.032e-03 6.685e-03 7.555e-03 3.056e-03 2.509e-03 4.961e-03 4.050e-02\n",
      " 1.294e-03 2.729e-03 1.322e-03 3.306e-03 7.999e-03 6.748e-04 7.929e-03\n",
      " 6.070e-04 3.196e-03 2.281e-02 7.496e-03 2.500e-02 5.943e-03 1.423e-03\n",
      " 5.448e-03 6.976e-03 1.301e-02 1.147e-02 2.808e-03 3.279e-03 2.380e-03\n",
      " 6.526e-03 3.131e-03 1.629e-03 7.517e-04 4.847e-03 1.534e-02 3.058e-03\n",
      " 1.830e-03 0.000e+00 0.000e+00 5.102e-04 2.215e-03 1.843e-02 2.235e-03\n",
      " 4.922e-03 4.585e-03 2.272e-03 1.015e-02 7.049e-03 2.711e-03 1.164e-02\n",
      " 9.126e-04 1.570e-04 0.000e+00 3.061e-03 1.005e-03 0.000e+00 3.587e-04\n",
      " 6.480e-04 2.539e-03 2.280e-03 6.578e-04 8.155e-03 4.765e-03 5.130e-03\n",
      " 3.262e-03 3.781e-04 2.488e-04 1.670e-04 2.434e-03 3.316e-04 1.914e-03\n",
      " 9.885e-04 1.400e-02 1.052e-02 1.284e-02 1.251e-02 2.096e-03 7.129e-03\n",
      " 5.668e-03 3.957e-03 5.036e-03 1.480e-03 3.315e-03 3.484e-02 1.282e-02\n",
      " 2.169e-02 1.225e-03 1.814e-03 2.074e-03 4.322e-03 2.683e-03 2.677e-02\n",
      " 8.757e-04 6.291e-04 1.565e-02 5.618e-03 5.684e-04 2.783e-03 2.375e-03\n",
      " 1.648e-03 8.000e-03 1.053e-02 2.012e-02 3.820e-03 2.668e-03 7.425e-03\n",
      " 2.934e-03 2.472e-03 5.803e-03 2.844e-03 4.684e-03 4.158e-03 2.225e-03\n",
      " 3.058e-04 1.378e-04 2.535e-04 3.401e-03 3.107e-03 3.278e-03 2.550e-03\n",
      " 1.293e-02 1.954e-02 3.311e-03 5.905e-03 9.418e-03 1.178e-03 5.104e-03\n",
      " 2.689e-02 2.625e-03 4.125e-03 3.503e-03 3.423e-03 3.393e-03 7.150e-03\n",
      " 5.787e-03 2.829e-03 2.199e-03 3.552e-03 2.015e-03 2.530e-03 2.632e-03\n",
      " 1.173e-03 4.422e-03 6.902e-04 3.558e-04 1.372e-02 3.030e-03 6.433e-03\n",
      " 2.637e-02 8.680e-03 7.893e-03 2.196e-03 1.249e-03 1.062e-02 1.953e-02\n",
      " 7.584e-02 1.447e-02 3.078e-02 2.652e-02 1.928e-02 5.919e-03 1.202e-03\n",
      " 2.497e-03 8.586e-03 2.550e-03 2.394e-03 6.899e-04 2.171e-03 1.935e-03\n",
      " 6.206e-03 5.243e-03 2.498e-03 4.371e-03 2.464e-03 6.139e-03 1.785e-03\n",
      " 6.888e-03 6.793e-03 2.847e-03 2.721e-03 5.629e-03 3.792e-03 3.258e-03\n",
      " 3.971e-03 2.712e-03 1.609e-03 6.922e-03 6.111e-03 3.422e-03 1.437e-03\n",
      " 9.424e-03 6.995e-03 2.588e-03 2.971e-03 4.094e-03 1.950e-03 2.703e-03\n",
      " 1.780e-02 1.286e-02 2.091e-03 9.643e-05 1.560e-03 2.958e-03 7.413e-03\n",
      " 1.108e-02 4.908e-04 2.650e-02 1.906e-03 2.894e-03 2.724e-03 2.795e-03\n",
      " 1.622e-02 4.238e-03 1.316e-02 9.419e-03 6.258e-03 3.114e-02 4.668e-03\n",
      " 7.986e-03 1.418e-03 4.715e-03 9.487e-03 5.441e-03 2.066e-03 3.997e-03\n",
      " 6.586e-03 1.972e-03 2.456e-03 2.191e-03 4.253e-03 3.435e-03 1.862e-03\n",
      " 4.775e-03 1.846e-03 3.671e-03 2.059e-03 3.208e-03 2.133e-03 4.397e-03\n",
      " 3.387e-03 2.820e-03 4.164e-03 7.255e-03 2.794e-03 3.423e-03 1.268e-03\n",
      " 4.916e-02 1.264e-02 2.572e-03 0.000e+00 1.645e-02 2.495e-03 5.847e-03\n",
      " 1.474e-02 1.903e-03 1.202e-02 1.587e-02 3.423e-03 4.841e-04 3.955e-03\n",
      " 7.940e-03 1.728e-03 2.852e-03 2.377e-03 5.708e-03 3.396e-02 2.728e-03\n",
      " 5.293e-03 2.101e-03 2.982e-03 2.945e-03 1.771e-02 5.590e-03 6.461e-03\n",
      " 3.384e-03 2.873e-03 3.668e-02 3.033e-02 6.535e-04 1.550e-03 7.893e-03\n",
      " 3.380e-03 1.184e-03 1.950e-03 3.054e-03 3.288e-03 5.480e-03 1.035e-02\n",
      " 3.562e-03 4.588e-03 8.590e-03 1.788e-03 2.950e-03 6.126e-03 1.175e-02\n",
      " 7.387e-03 3.900e-03 1.010e-02 5.501e-03 2.303e-03 1.230e-02 2.451e-03\n",
      " 2.414e-03 5.000e-03 8.529e-03 1.400e-03 3.195e-03 1.695e-02 3.281e-03\n",
      " 8.160e-03 2.971e-03 1.143e-02 3.663e-03 1.521e-03 4.988e-03 5.446e-04\n",
      " 1.558e-03 1.137e-02 3.321e-03 5.406e-03 2.414e-03 2.423e-03 2.707e-03\n",
      " 4.161e-03 3.336e-03 2.129e-03 3.349e-03 1.418e-03 1.215e-03 3.217e-02\n",
      " 1.137e-02 4.921e-03 7.984e-04 4.413e-03 5.792e-03 5.480e-03 2.854e-03\n",
      " 5.498e-03 5.627e-03 8.795e-03 3.416e-03 5.303e-03 1.455e-03 2.477e-03\n",
      " 5.232e-03 1.250e-02 3.437e-03 4.809e-04 2.657e-03 5.800e-04 2.022e-03\n",
      " 1.159e-03 1.955e-03 5.950e-03 4.561e-04 4.921e-03 3.970e-04 3.414e-03\n",
      " 4.335e-03 4.273e-03 2.593e-03 3.835e-03 8.077e-03 3.743e-03 2.646e-03\n",
      " 2.353e-03 2.353e-03 6.581e-03 2.569e-03 2.269e-03 5.067e-03 3.887e-03\n",
      " 5.417e-03 5.012e-03 2.426e-03 4.293e-04 0.000e+00 5.621e-05]\n",
      "[444  37 426 674 390  21 660 375 392 659  44 871 393 175 395 173 998 588\n",
      " 677 999 822 222 208 394 736 174 119 673 689 389 688 524 737 210 372 735\n",
      " 251 691 534 535  64 766 678 495 687 978 181 997 976 118 441 209 969 880\n",
      " 827 499 661 351 361 937 718 971 391  87 637 715 679 900 682 349 635  99\n",
      " 788 765 333 221 471 464 654 219  43 954 594 500 192 416 514 154 714 239\n",
      " 672 622 165 369 170 693 424 676 475 623 168 549  34 425 618 179 460 408\n",
      " 973 763 412 747 904 560 521  70 783  61 950 708 523 774  81  86 580 867\n",
      " 223 226 463 630 125 167 466 632  93 281 245 927 327 370 841 949 643 811\n",
      " 279 494 964 507  71 151 703 178 138 410 461 354 935 541 420 368 901 177\n",
      " 938 823 522 153 214 470 807 190 653 721 163 207 407  77  51 406 169 883\n",
      "  36 467 110 493 797 291 913 508 262 620 451 316 709 658  12 855 211 263\n",
      " 853 876 829 189 339 692 200 411 790 905 817 974 164  42 468 551 848  40\n",
      " 188 141 409 562 760  82 972 140 252 490 396 191  73 857  55 845 348 283\n",
      " 710 821 698 890 277 429 371 947 859 415 789 597 850 773 758 540 101 310\n",
      " 662 422 734 611 664 144 357  16 378 274 583 382 991 667  27 124 681 205\n",
      " 596  19 921 437  67 440  69 418 502  60 486 987 988 150  66 720 885 650\n",
      " 247 787  50 113 942 924  95 498 943 996 218  31 690 402 385 559 529 923\n",
      " 849 795 265 729 965 417 873 784 793 254 627 180 313 362 483 506 241 761\n",
      " 129 358 680 341  72 384 786 741 990 870 145 338 320 814 982 472 531 617\n",
      " 750 762 518 325 299 986 970 264 726 477 712 818 552 944 670 806 550 447\n",
      " 605 801  25 831 888 631 149 246 487 480 504 719 459 505 865 832 435 648\n",
      " 233 383 862 757  18 731 503 553 800 340 884 958 346 469 897 123 104 298\n",
      " 830 114 328 287 336 728  89 892 914  46 204 824 932 815   2  13 891 307\n",
      " 126 280  29 234 768 112 315 257 450 906 626 657 675 216  47 271  20 308\n",
      " 621 739 231 488 133 401 652 600 433 259 544  62 386 603 478 527 613 928\n",
      " 638 431 858 329 489 452 301 196 172 804 686 496 740 649 568 930 206 117\n",
      " 907 195 272 633 744 704 543 595 940 946 509 948 903 442 896 861 400 754\n",
      " 738 979 962 232 810 866 879 753 852 968 520  10 752 567 356  33 213 581\n",
      " 260 135 759 343 610 100 910  49 430 278 139 584 258 457  23 582 428 253\n",
      " 934 856 105 344 381 121 579   6 236 526 985  75 237 434 803 536 373  26\n",
      " 725 983 436 423   8 993 439 918 146 350 220 300   0  92 881 701 587 805\n",
      " 476 556 846  38 302 323 484 224 465 432 359  14 816 482 376 473 751 607\n",
      " 365 733 945 863 399 318 404 834 851 398 516 309 981 324 438 288 296  35\n",
      " 711 317 512 980   7 561 347 794 244 593 860 955 764 266 201  54 332 491\n",
      "  11 666 911 319  74 176 284 839 136 732 564 269 374 577   5 842 306  63\n",
      " 684 854 555 292 276 655 107 122  80 166 953 977 665 628 321 936 326 925\n",
      " 995  79 702 992   9 528 748 685 108  65 414 152 454 619 363 966 792 203\n",
      "  68 889 963 157 111 525 155 941 994 844 517 644 511 212 295 185 908 957\n",
      " 615 959 248 920 547 377   4 116  41 894  24 717 960 293 802 294 604 558\n",
      " 606 700 886  56 159 598 304 128 360 756 956 458 730 874 427 745 782 642\n",
      " 975 286 353 186 546 809 915 796 421  78 367 419 791 171 379 134 448 837\n",
      " 537 305 199 405  17 573 255 769 895 215 651 519 249 311 565 989 847 142\n",
      " 624 575  96 799 545  53 798 403 337 808 334 645 497 813   1 669  32 250\n",
      " 554 699 755   3 330 864 510 355 917  52 825  57 727 640 446 625 532 453\n",
      "  58 162 574 106 261 902 772 387 636 882 147 840 634 722 240 984 243 571\n",
      " 683 931  76 576 492 445 485 542 194 926 785 912 771 273 609 961  48 533\n",
      " 566  83 303 479 256 746 836 812 843 388 197 282 589 592 449  84 156 312\n",
      " 919  30 668  59 909 158 563 289 695 723 462 775 352  22 285 513 826 314\n",
      " 132 193 939 952 933 647 198 671 916 602 572 290 877 614 922 967 697  91\n",
      " 869 586 706 696 820 538 570 742 599  15 646 242 835 569 591 322 130 184\n",
      "  98 767 235 694 297 202 230 778  39 530 875 481 608 331 656 716 366 878\n",
      " 229 833 872 120 268 270 131 364 443 929 161 455 893 819 267 663 238 275\n",
      " 601 474  85 612 781 776 743 182 187 115 724  94 102 707 456 137 639 109\n",
      " 501 515 413 641 228 103 143 616 770 345 828 780 590 380 713 749 148  45\n",
      "  90 335 217 899 779 838  88 585 557 951  28 227 887 705 539 342 397 898\n",
      " 578 548 629 868 127  97 777 225 183 160]\n",
      "[4.655e-03 6.402e-03 3.254e-03 8.619e-03 5.374e-03 6.586e-03 5.539e-03\n",
      " 1.685e-03 6.660e-03 3.461e-03 2.209e-03 3.689e-03 2.698e-03 2.339e-03\n",
      " 2.331e-03 1.076e-02 2.851e-03 7.729e-03 2.441e-03 2.502e-03 2.616e-03\n",
      " 2.870e-03 1.327e-02 3.064e-03 3.058e-03 2.349e-03 1.433e-03 2.524e-03\n",
      " 2.092e-02 2.051e-03 2.160e-02 5.487e-03 3.529e-03 2.468e-02 1.107e-02\n",
      " 8.796e-03 2.425e-03 3.426e-03 2.559e-03 1.423e-02 3.255e-03 3.315e-03\n",
      " 7.778e-03 3.773e-03 5.000e-03 3.110e-02 3.101e-03 2.868e-03 8.328e-03\n",
      " 1.756e-03 2.774e-03 3.360e-03 1.133e-02 5.211e-03 2.942e-03 5.698e-03\n",
      " 7.419e-03 1.378e-02 1.246e-02 1.615e-02 1.001e-02 3.032e-03 2.543e-03\n",
      " 3.616e-03 4.486e-03 2.661e-03 3.427e-03 3.224e-03 5.728e-03 3.102e-03\n",
      " 2.740e-03 2.564e-03 2.552e-03 2.805e-03 4.621e-03 1.601e-03 1.469e-02\n",
      " 2.529e-03 3.388e-03 2.871e-03 1.139e-02 3.033e-03 2.758e-03 1.199e-02\n",
      " 1.456e-02 1.495e-02 6.602e-03 4.190e-03 3.401e-02 8.032e-03 2.166e-02\n",
      " 8.142e-05 4.711e-03 2.570e-03 2.266e-02 2.602e-03 9.362e-04 2.426e-03\n",
      " 1.345e-04 4.182e-03 3.625e-03 2.005e-03 1.653e-02 2.388e-02 3.674e-03\n",
      " 3.258e-03 5.325e-03 3.249e-03 3.205e-03 2.857e-02 2.644e-03 5.563e-03\n",
      " 3.095e-03 3.025e-03 4.084e-03 2.494e-03 9.174e-03 1.198e-02 3.018e-03\n",
      " 3.827e-03 1.767e-02 3.090e-03 5.881e-03 2.595e-03 3.094e-03 7.925e-03\n",
      " 2.141e-03 1.730e-02 1.972e-03 2.209e-03 4.613e-03 1.181e-03 5.408e-03\n",
      " 2.270e-03 7.330e-03 4.661e-03 2.193e-03 3.714e-03 3.241e-03 5.715e-03\n",
      " 3.373e-03 4.693e-03 4.643e-03 2.647e-02 7.918e-03 4.104e-03 2.226e-03\n",
      " 8.791e-03 3.026e-02 3.714e-03 2.664e-03 2.613e-03 3.110e-03 2.238e-03\n",
      " 2.950e-03 5.862e-03 1.195e-02 5.905e-03 6.062e-03 2.877e-03 2.123e-03\n",
      " 0.000e+00 4.175e-04 2.686e-03 7.181e-03 3.624e-03 2.882e-02 5.174e-03\n",
      " 6.695e-03 3.434e-03 3.073e-03 1.256e-02 2.125e-02 3.494e-03 3.882e-03\n",
      " 4.129e-03 2.925e-03 1.172e-02 7.748e-03 3.985e-03 3.240e-03 2.844e-03\n",
      " 1.366e-03 6.971e-04 4.882e-05 1.690e-03 3.349e-03 3.085e-02 3.834e-03\n",
      " 3.397e-03 3.619e-03 4.366e-03 4.164e-03 6.436e-03 2.396e-03 5.681e-03\n",
      " 2.990e-03 7.543e-03 7.223e-03 4.553e-03 2.663e-03 3.157e-03 9.429e-03\n",
      " 4.048e-03 2.802e-03 2.466e-03 2.402e-03 1.563e-02 7.418e-03 3.685e-03\n",
      " 5.406e-03 3.237e-03 3.868e-03 2.544e-03 2.258e-03 3.828e-03 1.758e-03\n",
      " 1.366e-02 3.290e-03 3.457e-03 1.652e-02 4.524e-03 3.004e-03 2.416e-03\n",
      " 2.820e-03 1.653e-02 4.888e-03 2.204e-03 1.085e-04 2.929e-03 1.635e-03\n",
      " 3.522e-03 4.989e-03 2.180e-03 6.587e-03 1.748e-02 2.783e-03 2.090e-03\n",
      " 1.175e-02 3.079e-03 1.153e-03 2.703e-03 1.149e-02 5.970e-03 3.175e-03\n",
      " 2.151e-03 3.119e-03 2.373e-03 5.514e-03 5.912e-03 5.515e-03 4.894e-03\n",
      " 2.968e-03 4.724e-03 2.026e-03 7.872e-03 8.911e-03 2.277e-03 3.544e-03\n",
      " 3.140e-03 2.369e-03 1.010e-02 2.431e-03 2.388e-03 2.854e-03 3.811e-03\n",
      " 2.625e-03 1.907e-02 1.822e-02 3.688e-03 1.808e-02 3.387e-03 2.620e-03\n",
      " 1.129e-02 2.705e-03 1.785e-02 5.158e-03 2.405e-03 2.402e-03 2.314e-03\n",
      " 2.064e-03 2.313e-03 1.177e-02 2.696e-03 6.659e-03 1.420e-02 7.009e-03\n",
      " 4.758e-03 2.127e-03 2.945e-03 1.034e-02 2.511e-03 4.213e-03 5.735e-03\n",
      " 6.680e-03 5.232e-03 4.545e-03 1.951e-04 4.430e-03 2.131e-03 2.157e-03\n",
      " 3.289e-03 3.417e-03 6.364e-03 6.070e-03 9.259e-03 4.983e-03 3.746e-03\n",
      " 2.874e-03 4.665e-03 2.228e-03 5.757e-03 8.133e-03 3.010e-03 1.175e-02\n",
      " 5.396e-03 3.855e-03 4.392e-03 3.496e-03 7.122e-03 3.041e-03 6.888e-03\n",
      " 1.346e-02 2.399e-03 4.853e-03 2.419e-03 2.551e-03 2.502e-03 2.580e-03\n",
      " 2.245e-03 6.088e-03 1.464e-02 7.472e-03 4.598e-03 5.243e-03 2.619e-02\n",
      " 3.131e-03 7.410e-03 2.549e-03 2.566e-03 5.724e-03 3.094e-03 3.105e-02\n",
      " 2.459e-03 3.770e-03 1.689e-02 4.852e-03 3.097e-03 1.608e-02 5.070e-03\n",
      " 6.973e-03 3.616e-03 9.542e-03 7.621e-03 3.965e-03 5.968e-03 3.014e-03\n",
      " 1.870e-03 2.374e-03 4.764e-03 1.225e-02 2.761e-03 2.517e-03 6.478e-03\n",
      " 1.728e-02 4.866e-03 6.954e-03 1.524e-02 3.190e-03 6.369e-03 4.835e-03\n",
      " 2.141e-03 4.276e-03 4.889e-03 3.646e-03 3.468e-03 2.880e-03 4.153e-03\n",
      " 2.542e-03 3.608e-03 2.875e-02 2.916e-03 3.175e-03 3.184e-03 3.111e-03\n",
      " 2.826e-03 3.125e-03 8.330e-03 8.840e-03 9.667e-03 3.168e-03 2.073e-02\n",
      " 2.955e-03 7.116e-03 3.855e-03 3.871e-03 2.435e-03 4.265e-02 6.895e-03\n",
      " 5.773e-03 4.576e-03 4.324e-03 2.580e-03 8.954e-03 7.812e-03 5.761e-03\n",
      " 6.960e-03 3.023e-03 1.553e-02 2.566e-03 5.311e-03 1.012e-02 3.220e-03\n",
      " 2.300e-02 3.591e-03 2.282e-03 4.489e-03 3.643e-03 3.952e-03 5.912e-03\n",
      " 2.243e-03 6.410e-03 2.292e-03 3.412e-03 3.721e-03 1.436e-02 2.909e-03\n",
      " 6.486e-03 2.373e-03 3.184e-03 3.053e-03 4.825e-03 3.210e-03 4.402e-03\n",
      " 5.147e-03 3.674e-03 3.774e-03 2.019e-03 4.778e-03 4.616e-03 3.293e-03\n",
      " 3.106e-03 2.825e-03 9.046e-03 3.766e-03 2.992e-03 2.287e-03 7.846e-03\n",
      " 3.806e-03 1.146e-02 2.057e-03 3.271e-03 2.434e-03 2.578e-03 5.296e-03\n",
      " 1.508e-02 9.003e-03 4.476e-03 2.978e-03 1.672e-03 3.284e-03 1.202e-02\n",
      " 3.303e-02 4.807e-03 2.905e-03 6.425e-03 2.954e-03 5.067e-03 3.374e-03\n",
      " 1.864e-03 2.418e-03 3.078e-03 4.487e-03 1.565e-02 4.465e-02 4.682e-03\n",
      " 4.612e-03 4.879e-03 2.233e-03 6.941e-03 2.442e-03 3.279e-03 3.572e-03\n",
      " 2.212e-03 3.007e-03 7.745e-03 4.304e-03 2.288e-03 3.756e-03 2.620e-03\n",
      " 1.977e-03 3.391e-03 1.003e-02 2.626e-03 1.947e-02 3.288e-03 2.022e-03\n",
      " 8.135e-03 2.130e-03 4.123e-03 3.749e-03 2.811e-02 2.125e-03 2.294e-03\n",
      " 1.954e-03 2.236e-03 2.876e-03 2.435e-03 2.554e-03 3.392e-03 8.460e-03\n",
      " 6.700e-03 6.014e-03 1.279e-02 2.826e-03 2.503e-02 4.617e-03 4.446e-03\n",
      " 2.298e-03 6.193e-03 3.553e-03 2.398e-03 3.011e-03 1.123e-02 2.851e-03\n",
      " 5.515e-03 5.179e-03 2.859e-03 3.522e-03 2.610e-03 1.413e-02 2.457e-03\n",
      " 6.478e-03 8.546e-03 1.157e-02 3.043e-03 4.485e-03 6.430e-03 5.403e-04\n",
      " 4.232e-02 5.366e-03 4.474e-03 2.486e-03 2.620e-03 4.702e-03 2.502e-03\n",
      " 5.741e-03 5.917e-03 3.707e-02 2.409e-03 3.472e-03 2.029e-03 1.987e-03\n",
      " 2.469e-03 6.486e-03 4.973e-03 2.463e-03 3.125e-02 6.631e-03 2.801e-03\n",
      " 2.981e-03 5.925e-03 3.536e-03 9.088e-03 6.147e-03 8.863e-03 6.678e-03\n",
      " 3.139e-03 1.763e-03 1.133e-02 1.285e-02 6.517e-03 8.856e-03 3.922e-03\n",
      " 1.871e-03 9.251e-03 8.950e-03 3.067e-03 3.837e-02 2.343e-03 4.021e-03\n",
      " 3.317e-03 4.500e-03 3.307e-03 6.152e-03 2.155e-02 1.849e-02 3.544e-03\n",
      " 3.017e-03 1.043e-02 2.769e-02 1.212e-02 1.009e-02 4.470e-03 3.082e-03\n",
      " 3.048e-03 4.217e-03 2.348e-03 6.151e-03 1.635e-02 4.737e-03 4.981e-04\n",
      " 1.007e-04 3.350e-03 6.003e-03 2.758e-03 4.622e-03 3.536e-03 2.024e-04\n",
      " 9.193e-03 4.066e-03 3.467e-03 2.799e-03 2.222e-03 1.240e-02 4.078e-03\n",
      " 2.501e-02 2.486e-03 4.591e-03 6.262e-03 2.476e-03 3.597e-03 2.757e-03\n",
      " 2.875e-03 2.729e-03 3.406e-03 3.204e-03 2.214e-03 5.684e-03 4.649e-02\n",
      " 2.894e-03 2.496e-03 3.104e-03 4.455e-03 9.284e-03 2.600e-03 4.681e-03\n",
      " 2.495e-03 2.415e-03 2.084e-02 6.812e-03 2.154e-02 5.584e-03 6.303e-03\n",
      " 2.124e-03 4.181e-03 7.407e-03 5.702e-04 3.762e-03 3.868e-03 2.567e-03\n",
      " 3.945e-03 7.019e-03 2.291e-03 6.084e-03 4.189e-03 2.103e-03 3.232e-03\n",
      " 2.426e-02 3.387e-03 6.645e-03 2.885e-03 3.639e-03 1.769e-02 2.607e-03\n",
      " 6.105e-03 4.082e-03 3.712e-03 8.191e-03 5.978e-03 5.737e-03 1.215e-02\n",
      " 1.835e-02 6.697e-03 3.210e-03 2.164e-03 2.971e-03 3.323e-03 3.071e-03\n",
      " 4.548e-03 2.532e-03 5.174e-03 4.540e-03 6.926e-03 2.588e-03 4.796e-03\n",
      " 4.486e-03 5.005e-03 3.044e-03 5.194e-03 1.724e-02 4.601e-03 7.876e-03\n",
      " 2.949e-03 1.435e-02 1.155e-02 1.526e-04 7.822e-05 2.878e-03 7.758e-03\n",
      " 6.055e-03 2.102e-03 2.591e-03 2.277e-03 3.005e-03 1.284e-03 1.009e-02\n",
      " 2.466e-02 2.557e-03 2.992e-03 2.468e-03 4.870e-03 2.078e-03 2.492e-02\n",
      " 7.627e-03 3.843e-03 3.119e-03 3.119e-03 4.813e-03 4.394e-03 2.260e-03\n",
      " 3.551e-03 8.928e-03 7.760e-03 2.273e-02 6.711e-03 4.566e-03 7.958e-03\n",
      " 2.901e-03 2.279e-03 6.355e-03 2.407e-03 2.504e-03 2.103e-03 2.077e-02\n",
      " 3.018e-03 2.333e-03 3.926e-03 3.657e-03 1.900e-03 5.855e-03 2.872e-03\n",
      " 1.501e-02 1.853e-02 1.580e-03 2.356e-03 4.233e-03 2.932e-03 5.790e-03\n",
      " 2.394e-02 2.400e-03 2.191e-03 3.376e-03 2.693e-03 2.674e-03 1.203e-02\n",
      " 5.613e-03 2.595e-03 1.145e-02 3.072e-03 2.933e-03 2.134e-03 2.184e-03\n",
      " 2.355e-03 5.621e-03 3.065e-03 2.601e-03 1.317e-02 3.544e-03 5.989e-03\n",
      " 9.153e-03 9.944e-03 5.924e-03 1.968e-03 3.293e-03 7.872e-05 1.290e-03\n",
      " 4.570e-03 0.000e+00 1.922e-03 1.298e-03 1.483e-03 1.706e-03 3.112e-03\n",
      " 2.807e-03 3.811e-03 6.069e-03 3.848e-03 3.698e-03 2.843e-03 2.927e-03\n",
      " 9.458e-03 5.648e-03 2.759e-03 2.678e-03 2.591e-03 3.146e-03 2.554e-03\n",
      " 6.988e-03 8.211e-03 5.098e-03 3.622e-03 3.003e-03 4.943e-03 5.396e-03\n",
      " 2.718e-03 3.029e-03 1.998e-03 2.249e-03 6.106e-03 3.115e-03 3.037e-03\n",
      " 7.129e-03 2.556e-03 2.071e-03 1.966e-03 2.690e-03 2.782e-03 3.157e-03\n",
      " 1.869e-02 2.100e-02 2.473e-03 3.949e-03 3.743e-03 2.009e-03 5.082e-03\n",
      " 1.118e-02 3.311e-03 3.040e-02 3.195e-03 5.100e-03 3.304e-03 3.777e-03\n",
      " 1.617e-02 4.812e-03 3.191e-03 1.516e-03 1.539e-03 1.916e-02 1.387e-03\n",
      " 3.533e-03 3.266e-03 5.886e-03 1.191e-02 8.743e-03 2.636e-03 4.327e-03\n",
      " 1.506e-02 4.494e-03 2.375e-03 4.683e-03 1.472e-02 1.161e-02 4.261e-03\n",
      " 1.448e-02 2.042e-03 2.268e-03 2.291e-03 5.110e-03 2.310e-03 4.419e-03\n",
      " 5.515e-03 2.373e-03 2.338e-03 1.006e-02 3.021e-03 2.719e-03 2.740e-03\n",
      " 9.765e-04 3.042e-04 3.069e-03 7.823e-03 1.622e-02 2.363e-03 4.873e-03\n",
      " 2.089e-02 2.689e-03 7.912e-03 9.187e-03 3.670e-03 5.139e-03 4.091e-03\n",
      " 9.150e-03 2.003e-03 3.441e-03 2.059e-03 4.586e-03 3.892e-02 5.047e-03\n",
      " 8.474e-03 5.475e-03 4.188e-03 3.314e-03 2.110e-02 3.396e-03 5.694e-03\n",
      " 3.139e-03 2.720e-03 7.090e-03 5.790e-03 2.562e-03 2.592e-03 1.084e-02\n",
      " 1.160e-02 4.806e-03 2.590e-03 3.881e-03 3.866e-03 5.740e-03 1.289e-02\n",
      " 4.883e-03 6.328e-03 9.112e-03 2.940e-03 1.877e-03 4.978e-03 7.309e-03\n",
      " 5.836e-03 4.748e-03 1.270e-02 5.471e-03 2.345e-03 1.119e-02 3.172e-03\n",
      " 3.285e-03 1.919e-03 7.795e-03 2.594e-03 5.623e-03 7.827e-03 2.674e-03\n",
      " 9.491e-03 3.158e-03 1.366e-02 2.494e-03 2.368e-03 5.492e-03 3.769e-03\n",
      " 2.702e-03 1.006e-02 4.937e-03 5.052e-03 2.323e-03 3.251e-03 6.823e-03\n",
      " 4.601e-03 3.439e-03 2.966e-03 4.479e-03 2.686e-03 2.751e-03 6.460e-03\n",
      " 1.262e-02 4.601e-03 2.848e-03 2.705e-03 1.204e-03 6.192e-03 1.964e-03\n",
      " 4.050e-03 4.501e-03 5.000e-03 1.831e-03 4.654e-03 5.278e-03 2.282e-03\n",
      " 8.096e-03 1.286e-02 5.000e-03 2.761e-03 3.118e-03 2.557e-03 4.626e-03\n",
      " 1.513e-02 2.341e-03 3.158e-02 3.525e-03 2.619e-02 2.777e-03 6.090e-03\n",
      " 1.816e-03 3.878e-03 3.127e-03 4.486e-03 3.472e-03 2.466e-03 2.914e-03\n",
      " 2.667e-03 2.345e-03 8.212e-03 2.797e-03 4.567e-03 4.310e-03 5.754e-03\n",
      " 2.862e-03 1.662e-02 6.260e-03 1.364e-02 4.012e-03 3.327e-03]\n",
      "[778 161 184 697 775  91 602 228  98 696 297 608 869 162 601 538 647 183\n",
      "  96 868 240 131 956 705 776 780 182 839  26 781 836 837 744  75 230 459\n",
      "   7 185 782  49 216 568 980 962 469 357 574 914 739 925 779 504 958 815\n",
      " 773 128 490 552 807 883 101 824 437 496 254 551 855  29 450 885 280 814\n",
      " 712 237 701 733 656 160 644 502 288 498 299 761 371 126 245 300 675 233\n",
      " 762 751 136 227  10 129 483 627 613 146 310 478 505 153 420 329 808 214\n",
      " 720 856 133 703 257 729 965 415 446 487 857 653 422 503 518 859 281 279\n",
      " 942  14 736 863  13 974 579 921 988 597  25 763 745 873 935 260 247 862\n",
      " 428 358 849 263 194 521 323 750 206 278 277 731 549 638 223 470 325  36\n",
      "  97 262 452 507 396  18 480 531 343 556 205 985 710 553 821 620 617 542\n",
      " 934 115 637 631 327 545  19 732 291 362  27  77 680 378  62 213 338 326\n",
      "  72 797 508 813 971 708  38 900  71 339 409 650  93 453 402 328 684 905\n",
      " 795 702 901 927 757 123 635 766  95 664 529 151  20 543 489 272 266 493\n",
      " 845 110  65 200 150 987 930 754 794 163 949 876 816 753 283  12 938 241\n",
      " 274 955 805 866 897 624 867  70 950 622  82 605 793 969 361  50 978 817\n",
      " 236 990 612 559 204  73 784 224 442 514 385 789 181 954 524  16 264 527\n",
      " 994  47  21  79 741 308 623 506 159 698 376 661 630 728 464 426 986 381\n",
      " 176 790 229 747 760 913  54 289 693 154 466 392 947 252 676 458 560 196\n",
      " 709 445 802 222 704 484 313 522 356 588 735 118 865 407 113 806  61  81\n",
      " 811 320 535 688 595 430  24  23 765 577 870 678 759 170 471 239 594 121\n",
      " 124 341 112 347  46  69 632 441 152 384 783 810 970 246 716 717 386 982\n",
      " 336 896 567 259 796 818 201 932 390 923 382 244 429 383 368 835 829 626\n",
      " 108 674 432 412  67 657 211 180 138 107 943   2  40 105 841 451 481 460\n",
      " 924 495 301 218 440 774 831 583 827 892  41 581 677 999 186 603  51 140\n",
      " 468 752 659 271  78 491 509 894 189 625 423 302  37  66 169 946 884 219\n",
      "   9 611 375 550 984 173 318 231 528 976  32 840 607 562 768 258 587 721\n",
      " 520 482 414 621 379  63 351 190 801 165 100 662 417 374 738 879 104 435\n",
      " 209 269  11 788 667 149 137 424 823 307 500 488 648 444 937 344  43 436\n",
      " 832 448 265 785 119 215 188 715 787 316 394 907 212 649 395 981 906 174\n",
      " 573 737 651 822 418 354 179 998 580 203 959 610 615 666 114 881 145 499\n",
      " 175 377 192 645  99 891 655  87 292 596 746 853 372 486 992 401 846 191\n",
      " 317 719 433 860 298 517 633 593 541 457 948 536 983  64 686 472 416 848\n",
      " 582 960 221 682 296 679 199 726 991 777 400 886 618 333 953 945 691 476\n",
      " 130 439 516  74 606 972 142 963   0 135 309 636 475 850 141 544  92 253\n",
      " 600 918 287 359 438 685 904 463 834 718 431 370 346 324 365 711 874 477\n",
      " 910 226 373 251 940 803 555 915 306 232 961  44 968 687 888 941 467 349\n",
      " 825 800 830 858 880 434 276 681 167 526 689  53 295 334 964 454 410 106\n",
      " 540   4 315 804 210 132 920 890  31 936 248 250 861 525   6 111 642 756\n",
      " 764 928 792 195 628 895  55 139 340  68 293 670 908 546 993 311 405 399\n",
      " 748 899 917 740 155 122 842 157 249 419 547 772 561 355 243 669 769 604\n",
      " 512 700 158 786 304 654 330 979 665 809 564 598 584 957 519 996 619 643\n",
      " 911 730 303 369   1 421 465 537 193 951 532 363 554 427 571   5 234  86\n",
      " 558 660 284   8 566 294 168 673 511 725 640 944 321 398 683 479 366 406\n",
      " 350 798 286 652 898 393 319 812 164 198 916 134 646 337 208  56 332 197\n",
      " 353 714  17 485 178 699 723  42 926 404 871 929 447 255 692 877 144 125\n",
      " 727  89 966 312 497 668 799 989  48 387 510 889 533   3 844 147  35 388\n",
      " 572 565 256 722 576 403 456 443 563 912 882 770 116 878 609 575 305 634\n",
      " 202 791 931 352 389 771  60 492 864 939 592 706 261 411 290 589  15 902\n",
      "  34 826 922 523 273  52 569  80 758 449 242 695 534 903 852 177 238 314\n",
      " 282 843 156 117  83 461 755 591 671 360 614  58 171 952 919 513 570 967\n",
      " 909 767  22 322 997 217 933  57 530 285  39 694 425 854  84 331  76 851\n",
      "  85 742 847 455 973 367 408 207 473 348  59 833 872 599 220 102 225 995\n",
      " 345 690 364 127 235 120 663 275 270 268 672 586 743 819 267 838 494 391\n",
      " 734 639 875  28 820 893 172 641 585  30  90  94 724 413 103 749 658 707\n",
      "  33 713 616 515 335 977 143 590 501 109 380 166 148 828 187 342  45 557\n",
      " 975 462  88 548 578 887 539 397 474 629]\n",
      "[[160 183 225 ... 426  37 444]\n",
      " [629 474 397 ... 184 161 778]]\n",
      "Cluster 0: [chocolate] [coffee] [dark] [roasted] [black] [brown] [stout] [nice] [like] [malt]\n",
      "Cluster 1: [nice] [hops] [good] [light] [sweet] [malt] [like] [bit] [hop] [white]\n"
     ]
    }
   ],
   "source": [
    "copy = result.copy()\n",
    "for x in range(len(result)):\n",
    "    print(result[x])\n",
    "    temp = np.argsort(result[x])\n",
    "    print(temp)\n",
    "    #temp.astype(int)\n",
    "    copy[x] = temp[::-1]\n",
    "    #final.append(list(temp[::-1]))\n",
    "copy = copy.astype(int)\n",
    "print(copy)\n",
    "print_cluster_features(vectorizer, copy, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3befb0848e0a08387a7ba63ad0b845e0",
     "grade": false,
     "grade_id": "cell-11b6922067d8ad4e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['chocolate',\n",
       "  'coffee',\n",
       "  'dark',\n",
       "  'roasted',\n",
       "  'black',\n",
       "  'brown',\n",
       "  'stout',\n",
       "  'nice',\n",
       "  'like',\n",
       "  'malt'],\n",
       " ['nice',\n",
       "  'hops',\n",
       "  'good',\n",
       "  'light',\n",
       "  'sweet',\n",
       "  'malt',\n",
       "  'like',\n",
       "  'bit',\n",
       "  'hop',\n",
       "  'white']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "def answer_kmeans_review():\n",
    "    #Optimal K-value of 2\n",
    "    #kmeans = KMeans(init=\"k-means++\", max_iter=100, n_init=1, n_clusters=2, random_state=42).fit(X)\n",
    "    \n",
    "    result = [['chocolate', 'coffee', 'dark', 'roasted', 'black', 'brown', 'stout', 'nice', 'like', 'malt'],\n",
    "             ['nice', 'hops', 'good', 'light', 'sweet', 'malt', 'like', 'bit', 'hop', 'white']]\n",
    "              \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return result\n",
    "\n",
    "answer_kmeans_review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "925e21a57199a80843d9ebff45125097",
     "grade": true,
     "grade_id": "cell-17e8ddcf0455395f",
     "locked": true,
     "points": 25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_kmeans_review()\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q1: Your function should return a list (of string lists). \"\n",
    "assert np.array([type(elt) == list for elt in stu_ans]).all(), \"Q1: each cluster summary should be a list (of terms).\"\n",
    "assert np.array([len(elt) == 10 for elt in stu_ans]).all(), \"Q1: each cluster summary should have exactly 10 terms.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Cluster labeling (25 points)\n",
    "\n",
    "It was fun to extract the words in the 'typical' review for each cluster using the cluster centroid. However, you may have noticed that some words were duplicated across clusters and weren't that specific to a single cluster. For example 'nice' and 'like' appear in more than one 'typical' review, which makes sense given that reviews express opinions no matter which cluster a brew might appear in.  \n",
    "\n",
    "Ideally, we'd like to find slighly better representative words for each cluster that are somewhat *specific* to that cluster and not others, i.e. words that distinguish reviews in that cluster from reviews in other clusters. To use the technical term, we want terms that are *discriminative* with respect to the clusters, not just descriptive.\n",
    "\n",
    "Recall that in dimensionality reduction we saw information gain (IG) as a feature selection criterion: it helps find features that distinguish one class from another class, improving prediction accuracy. Even though we don't have labels, we can use the same idea to find terms that are *special* to that cluster, that distinguish that cluster from the others. These IG-based distinctive terms will be our new cluster \"summary\". \n",
    "\n",
    "To obtain these improved terms, implement the following steps.  These instructions are a bit long, but that's to make the process as clear as possible.\n",
    "\n",
    "0. Run k-means again (using same algorithm settings as Q1) with the same beer review collection, **but this time setting k=7 (seven) to give a slightly more diverse set of clusters to start with**.\n",
    "\n",
    "\n",
    "1. Get the cluster-term matrix of cluster centers $L$ from the result of k-means.  This should be an array with seven (7) rows, one row per cluster, and 1000 columns (one column per word/term in the vocabulary produced by the vectorizer).\n",
    "\n",
    "\n",
    "2. For each row $c$ in $L$,  (i.e. for each cluster)\n",
    "\n",
    "    a. For this cluster, we're going to compute a 'distinctive term score' for each of the terms/words in the vocabulary, to see how specific it is to this particular cluster. To do this, we're going to compute how surprised we are at the distribution $T_w$ of a word $w$ between this cluster vs all other clusters, compared to the distribution $T_c$ of the average word between this cluster $c$ and all clusters. If a word is a lot more likely to occur in this cluster than we would expect by chance, it is considered a more distinctive word for this cluster. This surprise factor is *information gain*.  Please see the link to the mini-lecture on information gain here: Starting @ 38:15 https://www.coursera.org/learn/siads542/lecture/y6wx8/recording-of-siads-542-office-hour-with-kevyn-collins-thompson-on-21-01-28-10-58\n",
    "    \n",
    "    More formally, we define the variable $T_c$ to have two possible values: \"in this cluster $c$\" and \"in any other cluster\". Each of these outcomes has a probability when we're talking about where words occur. Thus, $T_c$ is a numpy array of shape (2, ). For example, suppose we pick a word from the vocabulary at random and find that on average, $T_c$ = [0.25, 0.75]. In other words, the chance on average that a word occurs in this cluster $c$ is 25%, and the chance of occurring in any other cluster is 75%.\n",
    "    \n",
    "    We have provided a function for you that makes it easy to compute $T_c$, or other statistics of words in this cluster $c$ compared to all other clusters. This is the provided function `one_vs_all_count_matrix`. As input, you provide the matrix $L$ along with the row $c$ you want to analyze. The output is a \"one versus all\" matrix $M$ that has shape (2, 1000): the first row has the weights of words in *this* cluster $c$ (which is the same as L[c, :]), and the second row has the aggregate summed weights of the words across all *other* clusters.  The columns are the same as $L$: one column per term in the vocabulary.\n",
    "    \n",
    "    b. To compute $T_c$, just compute $M$ and sum over all columns (axis = 1) so that you get a (2, ) array. Then to turn it into a probability distribution, normalize by computing `T_c = T_c / sum(T_c)`.\n",
    "    \n",
    "    c. Now loop through each column (term) $i$ of $L$. We'll call the term corresponding to the $i$-th column, term $w$. So for each term $w$ you'll compute the second part we need: the term-specific distribution $T_w$ for that specific term $w$ in this cluster vs all clusters. \n",
    "    \n",
    "    To compute $T_w$ is very simple: it's just the $i$-th column of the one-vs-all matrix $M$ (term weight in this cluster, term weight in all other clusters) but again normalized to turn it into a 2-element probability distribution by computing 'T_w = T_w / sum(T_w)'.  Like $T_c$, this should be an array of shape (2, ). \n",
    "    \n",
    "    Then use the provided `compute_distinctive_term_score` function to compute the information gain for that term. The first argument is the two-element probability distribution $T_c$ you computed for all words, and the second argument is the two-element term-specific probability distribution $T_w$ just for this term $w$.  Append the resulting score to a result list.  \n",
    "    d. After looping through all terms in (c) above, you should have a result list with 1000 entries, one per term, containing the distinctive score for each term, for that cluster $c$.\n",
    "    \n",
    "    e. Get the most distinctive terms for this cluster $c$ by sorting your array by descending score and selecting the top 5.\n",
    "\n",
    "\n",
    "3. Your function should return a list of string lists: the main list will contain, for each cluster, a string list containing the top *five* terms, sorted from highest to lowest modified distinctive term score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def compute_distinctive_term_score(T, T_a):   \n",
    "    # First compute information gain.\n",
    "    IG = entropy(T) - entropy(T_a) \n",
    "          \n",
    "    # if it's high IG, but not for this class, we want to penalize,\n",
    "    # so flip the IG negative.  We do this because these are terms those whose *absence* is notable, \n",
    "    # but we don't care about those for purposes of this assignment and so we give them\n",
    "    # a score that guarantees we won't rank them highly.\n",
    "    if (T_a[0] < T_a[1]):\n",
    "        score = -IG  \n",
    "    else:\n",
    "        score = IG\n",
    "    return score\n",
    "\n",
    "# create a 1-vs-all two-class matrix for each cluster\n",
    "def one_vs_all_count_matrix(m, index):\n",
    "    # row zero is the selected row\n",
    "    row0 = m[index, :]\n",
    "    # row one is the other rows summed\n",
    "    row1 = np.vstack((m[0:index, :], m[index+1:, :])).sum(axis=0)\n",
    "              \n",
    "    result = np.vstack((row0, row1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0\n",
    "kmeans = KMeans(init=\"k-means++\", max_iter=100, n_init=1, n_clusters=7, random_state=42).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1\n",
    "L = kmeans.cluster_centers_\n",
    "L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 iterating loop\n",
    "final = []\n",
    "for c in range(len(L)):\n",
    "    M = one_vs_all_count_matrix(L, c)\n",
    "    T_c = M.sum(axis=1)\n",
    "    T_c = T_c / sum(T_c)\n",
    "    #print(T_c)\n",
    "    temp = []\n",
    "    for i in range(len(L[0])):\n",
    "        w = M[:,i]\n",
    "        T_w = w/sum(w)\n",
    "        result = compute_distinctive_term_score(T_c, T_w)\n",
    "        temp.append(result)\n",
    "    #temp.sort(reverse=True)\n",
    "    final.append(temp)\n",
    "#print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[955 878 929 ... 735 535  37]\n",
      " [737 735 736 ... 744 745 415]\n",
      " [985  42 984 ... 233  37 778]\n",
      " ...\n",
      " [144 865  93 ... 697 954 162]\n",
      " [588  61 970 ... 735 228 733]\n",
      " [778 656 481 ... 426 441 499]]\n",
      "Cluster 0: [vintage] [sugar] [toffee] [nuts] [barleywine]\n",
      "Cluster 1: [pumpkin pie] [pumpkin ale] [pumpkin beer] [pumpkin] [orange color]\n",
      "Cluster 2: [woody] [apple] [wood] [peach] [apples]\n",
      "Cluster 3: [pale malt] [light bodied] [inch head] [grain] [pils]\n",
      "Cluster 4: [caramel malt] [sticky lacing] [bitter hops] [hop] [hop bite]\n",
      "Cluster 5: [marzen] [bar] [went] [sam] [oktoberfest]\n",
      "Cluster 6: [roasted coffee] [opaque] [imperial] [dark brown] [colored head]\n"
     ]
    }
   ],
   "source": [
    "#final_terms = []\n",
    "final_copy = final.copy()\n",
    "\n",
    "for i in range(len(final_copy)):\n",
    "    #print(final[i])\n",
    "    temp = np.argsort(final[i])\n",
    "    #print(temp)\n",
    "    final_copy[i] = temp[::-1]\n",
    "\n",
    "#final_copy = final_copy.astype(int)\n",
    "#final_copy[0].type\n",
    "final_results = []\n",
    "for j in final_copy:\n",
    "    final_results.append(list(j))\n",
    "final_results = np.array(final_results)\n",
    "\n",
    "print(final_results)\n",
    "print_cluster_features(vectorizer, final_results, 7, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2777370607781485\n",
      "0.2777370607781485\n"
     ]
    }
   ],
   "source": [
    "print(max(final[0]))\n",
    "\n",
    "print(final[0][955])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "288d14b04a910a8531f8da1c8fb9e63d",
     "grade": false,
     "grade_id": "cell-f71840b128d02b5c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['vintage', 'sugar', 'toffee', 'nuts', 'barleywine'],\n",
       " ['pumpkin pie', 'pumpkin ale', 'pumpkin beer', 'pumpkin', 'orange color'],\n",
       " ['woody', 'apple', 'wood', 'peach', 'apples'],\n",
       " ['pale malt', 'light bodied', 'inch head', 'grain', 'pils'],\n",
       " ['caramel malt', 'sticky lacing', 'bitter hops', 'hop', 'hop bite'],\n",
       " ['marzen', 'bar', 'went', 'sam', 'oktoberfest'],\n",
       " ['roasted coffee', 'opaque', 'imperial', 'dark brown', 'colored head']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "def answer_cluster_labeling():\n",
    "    result = [['vintage', 'sugar', 'toffee', 'nuts', 'barleywine'],\n",
    "             ['pumpkin pie', 'pumpkin ale', 'pumpkin beer', 'pumpkin', 'orange color'],\n",
    "              ['woody', 'apple', 'wood', 'peach', 'apples'],\n",
    "              ['pale malt', 'light bodied', 'inch head', 'grain', 'pils'],\n",
    "              ['caramel malt', 'sticky lacing', 'bitter hops', 'hop', 'hop bite'],\n",
    "              ['marzen', 'bar', 'went', 'sam', 'oktoberfest'],\n",
    "              ['roasted coffee', 'opaque', 'imperial', 'dark brown', 'colored head']]\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return result\n",
    "\n",
    "answer_cluster_labeling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "74ff9c35adaa2c4f92407988cf786319",
     "grade": true,
     "grade_id": "cell-9175ae40475d7568",
     "locked": true,
     "points": 25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_cluster_labeling()\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q2: Your function should return a list (of string lists). \"\n",
    "assert np.array([type(elt) == list for elt in stu_ans]).all(), \"Q2: each cluster summary should be a list (of terms).\"\n",
    "assert np.array([len(elt) == 5 for elt in stu_ans]).all(), \"Q2: each cluster summary should have exactly 5 terms.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "attachments": {
    "dendrogram_labeled.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Hierarchical clustering (25 points)\n",
    "\n",
    "Now we're going to use the beer names that correspond with the reviews to do a little hierarchical beer clustering according to their review similarity (based on the bag of words model used in the first question).\n",
    "\n",
    "We're actually going to rely on scipy for this question: specifically, scipy.cluster.hierarchy and its functions `ward(.)` and `dendrogram(.)`.\n",
    "\n",
    "Note that the cluster-term matrix returned by the vectorizer is sparse. However, we need a dense array to use the ward() function, so we densify the array, i.e. cls = ward(X.toarray())\n",
    "\n",
    "The structure returned by `ward` contains the linkage structure computed by the bottom-up, agglomerative clustering, and looks like this:\n",
    "\n",
    "`[[ 7.     8.     0.947  2.   ]\n",
    "  [ 0.     1.     1.111  2.   ]\n",
    "  [ 2.    10.     1.15   3.   ]\n",
    "  [ 5.    11.     1.204  3.   ]\n",
    "  [ 3.     6.     1.219  2.   ]\n",
    "  [ 4.    14.     1.33   3.   ]\n",
    "  [ 9.    13.     1.34   4.   ]\n",
    "  [15.    16.     1.509  7.   ]\n",
    "  [12.    17.     1.728 10.   ]]`\n",
    " \n",
    "Let's call this structure table `T`. It describes a tree, where each node of the tree has a unique integer called its \"node index\".  The columns of `T` are:\n",
    "- node index of first child\n",
    "- node index of second child\n",
    "- cluster distance at which the clusters were merged\n",
    "- number of instances in the resulting new cluster, after the merge.\n",
    "\n",
    "It's useful to understand and be able to compute things with this structure if needed. \n",
    "\n",
    "A cluster with a node index of less than N corresponds to a leaf node at the bottom of the tree, i.e. one of the original observations: the original data instances have node indexes of 0 through N-1 where N is the total number of instances (data points). They are considered to be in their own 1-instance cluster and are not included in `T`.\n",
    "\n",
    "In row i of `T`, two clusters with node indices T[i, 0] and T[i, 1] are combined to form a new cluster with node index N + i. \n",
    "The cluster algorithm's computed distance (e.g. Ward's distance) between clusters T[i, 0] and T[i, 1] is given by T[i, 2]. \n",
    "The fourth column value T[i, 3] represents the number of original observations (leaf nodes) that are in the newly formed cluster.\n",
    "\n",
    "The tree corresponding to this structure, with all node indices labeled, is shown here:\n",
    " ![dendrogram_labeled.png](attachment:dendrogram_labeled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And here are the first 30 beer reviews clustered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.    27.     0.849  2.   ]\n",
      " [ 8.    26.     0.955  2.   ]\n",
      " [ 7.    31.     1.052  3.   ]\n",
      " [ 3.    16.     1.054  2.   ]\n",
      " [17.    22.     1.056  2.   ]\n",
      " [ 6.    14.     1.065  2.   ]\n",
      " [ 9.    29.     1.147  2.   ]\n",
      " [10.    25.     1.172  2.   ]\n",
      " [18.    20.     1.176  2.   ]\n",
      " [11.    23.     1.204  2.   ]\n",
      " [21.    28.     1.223  2.   ]\n",
      " [ 5.    38.     1.243  3.   ]\n",
      " [ 4.    24.     1.245  2.   ]\n",
      " [13.    40.     1.272  3.   ]\n",
      " [ 0.    43.     1.301  4.   ]\n",
      " [ 2.    19.     1.321  2.   ]\n",
      " [12.    42.     1.344  3.   ]\n",
      " [15.    45.     1.372  3.   ]\n",
      " [35.    37.     1.404  4.   ]\n",
      " [39.    47.     1.415  5.   ]\n",
      " [41.    44.     1.436  7.   ]\n",
      " [34.    46.     1.46   5.   ]\n",
      " [49.    50.     1.526 12.   ]\n",
      " [36.    52.     1.572 14.   ]\n",
      " [51.    53.     1.607 19.   ]\n",
      " [33.    54.     1.655 21.   ]\n",
      " [48.    55.     1.685 25.   ]\n",
      " [30.    56.     1.691 27.   ]\n",
      " [32.    57.     1.906 30.   ]]\n"
     ]
    }
   ],
   "source": [
    "(X, vectorizer, review_instances) = get_beer_reviews_vectorized(30, (1,2), 1000)\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "plt.figure()\n",
    "cls = ward(X.toarray())\n",
    "dendrogram(cls)\n",
    "plt.show()\n",
    "print(cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay - here's what you need to do for this question. The goal is to compare how the large clusters found by hierarchical clustering compare with the ones you found using k-means clustering.\n",
    "\n",
    "**Step 1.** Call `(X, vectorizer, review_instances) = get_beer_reviews_vectorized(1000, (1,2), 1000)` to get the review-term matrix of sparse vectors corresponding to the first 1000 reviews in the dataset (and using the top 1000 bigram terms by frequency).\n",
    "\n",
    "**Step 2.** Use the scipy wards method to cluster the instances in X.\n",
    "\n",
    "**Step 3.** Use the resulting hierarchical cluster tree to get the reviews that fall into each of K clusters, where K = 2.\n",
    "\n",
    "The easiest way to do this question is to use the \"fcluster\" function that's part of the scipy.cluster.hierarchy package.\n",
    "See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html\n",
    "\n",
    "You should use the 'maxclust' option of fcluster to automatically find the merge distance threshold that splits the reviews into K clusters and returns the cluster label of each review.  Use K = 2 for the results you submit.\n",
    "\n",
    "**Step 4a** Write a loop that, for each cluster you found in Step 3, does the following:\n",
    "\n",
    "**Step 4b**  Use the loop index 0, 1, ... as the cluster ID. Within the loop, filter the results of `fcluster` to find only the instances that belong to the given cluster ID.  Recall that the numpy \"where\" function can return you the indices of an array that meet your condition (such as which cluster index an instance has). You can then use these indices to select all elements with a given cluster ID.\n",
    "\n",
    "**Step 4c.** Next within the loop, call the `compute_centroid` function to compute a centroid for the elements you found in Step 4b: the result will be a 'typical' review for that cluster -- essentially, a \"term cloud\" for that cluster's reviews. \n",
    "\n",
    "**Step 4d.** Next within the loop, find the top FIVE terms in the centroid having the *highest* weight (sorted by descending weight), and save the strings for those terms in a list.\n",
    "\n",
    "**Step 5.** Outside the loop, your function should return a list that contains TWO term lists (strings) corresponding to the K=2 clusters. Each term list should contain the top *five* terms in the cluster centroid you computed (sorted by highest to lowest term weight).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "#Step 1\n",
    "(X, vectorizer, review_instances) = get_beer_reviews_vectorized(1000, (1,2), 1000)\n",
    "\n",
    "#Step 2\n",
    "wards = ward(X.toarray())\n",
    "\n",
    "#Step 3\n",
    "clusters = fcluster(Z = wards, t = 2, criterion = 'maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(clusters.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.004 0.002 0.    0.    0.    0.    0.005 0.    0.    0.    0.002 0.\n",
      "  0.    0.    0.    0.007 0.01  0.    0.    0.    0.    0.006 0.    0.\n",
      "  0.009 0.    0.    0.    0.005 0.058 0.021 0.011 0.013 0.004 0.    0.007\n",
      "  0.    0.    0.009 0.    0.    0.    0.    0.    0.015 0.    0.    0.007\n",
      "  0.009 0.    0.    0.007 0.002 0.    0.    0.004 0.004 0.003 0.014 0.011\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.004\n",
      "  0.    0.    0.009 0.    0.03  0.    0.    0.    0.005 0.    0.    0.\n",
      "  0.052 0.025 0.    0.005 0.014 0.018 0.    0.014 0.002 0.    0.    0.002\n",
      "  0.    0.    0.    0.005 0.014 0.018 0.025 0.    0.    0.    0.    0.\n",
      "  0.    0.01  0.    0.    0.    0.    0.    0.    0.011 0.016 0.    0.01\n",
      "  0.007 0.002 0.    0.    0.    0.004 0.    0.    0.013 0.005 0.    0.\n",
      "  0.016 0.    0.004 0.005 0.    0.019 0.    0.    0.    0.    0.013 0.\n",
      "  0.    0.015 0.006 0.01  0.    0.006 0.028 0.    0.005 0.    0.    0.\n",
      "  0.    0.003 0.    0.    0.    0.    0.    0.    0.005 0.    0.    0.\n",
      "  0.055 0.    0.004 0.006 0.    0.    0.003 0.019 0.005 0.012 0.006 0.014\n",
      "  0.042 0.004 0.    0.    0.    0.    0.    0.    0.    0.037 0.01  0.004\n",
      "  0.007 0.    0.    0.    0.    0.01  0.01  0.009 0.009 0.    0.002 0.01\n",
      "  0.    0.009 0.01  0.004 0.034 0.012 0.    0.    0.    0.002 0.    0.\n",
      "  0.    0.    0.    0.    0.026 0.    0.    0.005 0.008 0.009 0.    0.\n",
      "  0.006 0.    0.    0.    0.    0.    0.    0.014 0.    0.    0.004 0.\n",
      "  0.01  0.006 0.    0.    0.006 0.    0.    0.03  0.    0.    0.    0.\n",
      "  0.032 0.    0.002 0.    0.006 0.007 0.    0.019 0.01  0.    0.    0.005\n",
      "  0.    0.    0.014 0.    0.    0.003 0.009 0.    0.004 0.    0.    0.004\n",
      "  0.004 0.    0.    0.    0.    0.    0.    0.003 0.003 0.    0.    0.\n",
      "  0.    0.005 0.    0.009 0.    0.    0.009 0.005 0.    0.006 0.    0.008\n",
      "  0.    0.    0.    0.007 0.046 0.    0.008 0.    0.    0.006 0.009 0.007\n",
      "  0.    0.    0.002 0.    0.    0.    0.011 0.    0.    0.044 0.    0.\n",
      "  0.    0.009 0.    0.002 0.011 0.    0.    0.    0.    0.    0.    0.011\n",
      "  0.007 0.    0.002 0.017 0.006 0.021 0.    0.012 0.007 0.    0.004 0.\n",
      "  0.    0.    0.038 0.    0.    0.002 0.014 0.    0.    0.    0.    0.\n",
      "  0.004 0.005 0.005 0.007 0.016 0.007 0.    0.    0.003 0.    0.023 0.007\n",
      "  0.004 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.036\n",
      "  0.    0.    0.007 0.012 0.    0.    0.    0.01  0.    0.    0.01  0.012\n",
      "  0.006 0.02  0.    0.016 0.    0.    0.035 0.    0.    0.    0.016 0.\n",
      "  0.004 0.005 0.    0.006 0.    0.    0.    0.    0.    0.    0.019 0.006\n",
      "  0.    0.003 0.007 0.    0.    0.    0.    0.    0.    0.018 0.007 0.\n",
      "  0.006 0.    0.    0.015 0.    0.006 0.    0.    0.    0.01  0.004 0.021\n",
      "  0.    0.008 0.    0.    0.006 0.01  0.008 0.006 0.006 0.    0.004 0.006\n",
      "  0.022 0.    0.    0.    0.    0.    0.009 0.01  0.004 0.    0.    0.\n",
      "  0.    0.006 0.    0.006 0.    0.    0.012 0.    0.    0.    0.013 0.006\n",
      "  0.01  0.008 0.    0.    0.    0.004 0.012 0.005 0.    0.01  0.    0.\n",
      "  0.    0.005 0.    0.    0.021 0.005 0.    0.    0.    0.002 0.005 0.\n",
      "  0.    0.022 0.    0.    0.024 0.01  0.012 0.    0.    0.    0.    0.\n",
      "  0.007 0.    0.    0.01  0.009 0.01  0.    0.    0.005 0.005 0.    0.018\n",
      "  0.006 0.    0.    0.009 0.003 0.007 0.    0.004 0.009 0.    0.031 0.\n",
      "  0.    0.    0.    0.    0.005 0.    0.    0.024 0.004 0.004 0.    0.\n",
      "  0.    0.    0.004 0.005 0.    0.004 0.005 0.008 0.003 0.    0.    0.015\n",
      "  0.    0.    0.005 0.005 0.004 0.014 0.    0.002 0.    0.    0.008 0.005\n",
      "  0.021 0.    0.005 0.    0.    0.    0.    0.    0.021 0.    0.    0.027\n",
      "  0.014 0.003 0.006 0.    0.    0.    0.006 0.032 0.003 0.    0.    0.\n",
      "  0.014 0.    0.    0.    0.006 0.    0.    0.008 0.005 0.    0.007 0.009\n",
      "  0.017 0.012 0.    0.    0.    0.    0.003 0.    0.007 0.    0.037 0.\n",
      "  0.    0.    0.008 0.01  0.    0.    0.    0.    0.017 0.    0.006 0.\n",
      "  0.011 0.    0.009 0.093 0.    0.    0.    0.    0.    0.    0.004 0.\n",
      "  0.004 0.015 0.    0.    0.    0.    0.039 0.    0.031 0.    0.01  0.004\n",
      "  0.008 0.    0.    0.    0.01  0.014 0.    0.    0.    0.    0.    0.007\n",
      "  0.    0.    0.    0.007 0.    0.    0.    0.157 0.008 0.    0.    0.\n",
      "  0.    0.    0.004 0.    0.    0.    0.005 0.    0.005 0.007 0.    0.005\n",
      "  0.    0.    0.013 0.013 0.    0.    0.017 0.    0.    0.007 0.    0.\n",
      "  0.016 0.    0.    0.005 0.    0.005 0.    0.01  0.006 0.011 0.004 0.\n",
      "  0.    0.    0.    0.    0.491 0.078 0.144 0.    0.    0.    0.    0.\n",
      "  0.011 0.    0.01  0.    0.    0.    0.    0.006 0.016 0.016 0.    0.002\n",
      "  0.    0.    0.    0.    0.006 0.    0.005 0.005 0.021 0.    0.011 0.\n",
      "  0.    0.    0.    0.    0.    0.013 0.011 0.005 0.007 0.003 0.01  0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.004\n",
      "  0.007 0.    0.    0.    0.    0.    0.012 0.    0.    0.005 0.    0.\n",
      "  0.    0.006 0.007 0.    0.    0.015 0.008 0.    0.    0.006 0.    0.\n",
      "  0.    0.    0.005 0.002 0.004 0.    0.003 0.    0.011 0.    0.003 0.004\n",
      "  0.    0.    0.004 0.    0.011 0.    0.028 0.008 0.    0.    0.    0.005\n",
      "  0.    0.054 0.    0.    0.004 0.014 0.011 0.    0.    0.    0.    0.004\n",
      "  0.    0.    0.    0.    0.    0.003 0.003 0.    0.    0.    0.    0.\n",
      "  0.    0.    0.037 0.138 0.006 0.014 0.015 0.017 0.    0.    0.    0.005\n",
      "  0.    0.005 0.    0.004 0.    0.    0.    0.    0.    0.    0.017 0.\n",
      "  0.    0.015 0.012 0.026 0.    0.006 0.    0.    0.    0.    0.004 0.\n",
      "  0.    0.024 0.    0.009 0.    0.    0.005 0.017 0.    0.    0.013 0.\n",
      "  0.003 0.003 0.    0.037 0.    0.006 0.    0.    0.    0.    0.003 0.006\n",
      "  0.    0.018 0.005 0.013 0.003 0.    0.    0.02  0.004 0.021 0.006 0.\n",
      "  0.011 0.    0.    0.    0.    0.    0.006 0.    0.    0.003 0.    0.004\n",
      "  0.    0.015 0.    0.    0.006 0.    0.003 0.004 0.004 0.    0.    0.\n",
      "  0.011 0.    0.008 0.006 0.    0.007 0.006 0.05  0.011 0.    0.    0.015\n",
      "  0.    0.    0.003 0.    0.    0.006 0.    0.019 0.024 0.006 0.008 0.\n",
      "  0.    0.    0.    0.    0.    0.    0.031 0.    0.026 0.    0.008 0.\n",
      "  0.003 0.    0.    0.    0.    0.004 0.    0.011 0.002 0.011 0.005 0.005\n",
      "  0.    0.    0.    0.   ]]\n",
      "[[499 579 580 581 582 583 585 586 591 577 592 597 598 599 601 602 603 605\n",
      "  606 593 573 572 570 530 534 537 539 540 541 542 543 545 546 550 551 552\n",
      "  553 556 561 562 564 565 609 614 615 616 653 655 657 661 662 663 666 667\n",
      "  668 669 670 672 673 674 676 677 678 681 682 652 529 651 647 617 619 621\n",
      "  623 624 625 628 629 630 631 633 635 637 640 641 642 643 644 645 650 526\n",
      "  523 522 405 407 410 412 413 414 415 416 417 420 423 424 425 426 427 428\n",
      "  431 433 434 404 436 403 400 369 373 374 375 376 377 378 379 380 381 382\n",
      "  384 385 388 389 390 392 393 398 401 683 438 440 490 491 492 494 495 498\n",
      "  998 500 503 504 506 507 511 512 513 514 515 517 518 488 439 484 482 444\n",
      "  446 447 453 457 458 459 460 461 465 466 467 468 470 472 473 475 476 477\n",
      "  483 684 685 687 876 880 882 883 884 885 887 888 890 892 893 896 897 899\n",
      "  902 904 906 907 908 875 909 873 871 841 842 843 844 847 848 849 850 851\n",
      "  852 853 860 861 862 864 866 868 869 870 872 840 912 918 964 966 971 972\n",
      "  973 974 975 976 977 979 981 983 985 986 987 988 990 996 997 963 917 961\n",
      "  958 923 925 926 927 928 929 931 932 934 936 938 939 941 945 946 947 949\n",
      "  952 957 960 367 838 836 729 730 731 733 735 736 737 738 742 744 745 746\n",
      "  747 749 753 755 756 757 758 728 759 727 722 688 689 691 694 696 697 700\n",
      "  701 703 704 706 707 709 710 712 714 719 720 721 723 837 760 768 800 802\n",
      "  803 804 805 809 811 813 816 817 819 821 824 825 826 828 830 831 835 799\n",
      "  767 796 792 769 770 771 772 773 774 775 776 777 778 781 782 783 784 785\n",
      "  787 788 790 791 795 366 999 184 264 105 262 106 261 107 258 108 110 111\n",
      "  112 113 255 114 253 115 118 122 251 250 123 124 249 126 248 127 246 265\n",
      "  245 104 268  78 290  79  81  82 288 287  83 286 285  86 282 281 280 279\n",
      "  195  90 277  93 274  94 273  96  97 271  98 103 267 130 131 243 162 163\n",
      "  219 218 217 165 216 166 215 167 214 169 212 211 210 172 173 182 183 204\n",
      "  185 186 187 201 188 193 194 221 222 161 160 242 133 136 138 239 139 140\n",
      "  237 236 141 143 234 233  77 144 231 148 230 151 153 154 155 229 156 227\n",
      "  226 158 159 232 292 278  75 337  27 334 333 332 331 330 329  34 326  36\n",
      "   37 324  39 323 293  40  41  42 320 319  43 317  45 316 315  46  26  49\n",
      "   25  22   2   3 359 358 357 356 355   4   5 352 351   7   8 349 348   9\n",
      "  347  11  12  13 345  14  17  18  19 342  20  23  50 322 196 296  70 298\n",
      "  308 307 305  60  61  72  62 302  64 301 300  65  69  68  66  63  54  67\n",
      "  312  53  73 313  52   1 501  95 338 327 121 992 807 254 571  92  10 314\n",
      "  743 202 213 353 765 368 984 900 942 157 916 421 532 284 810 814 901 846\n",
      "  269 283 560  57 933 845 174 618 596 962 910 589 839 646 442 408 989 170\n",
      "  372 276 125 454 464  71 554 818 557 659 346 485 815 535  33 238 718 832\n",
      "  920 886 568  56 867 360 935 134 686 808 181 272 548 191 943 275  55   0\n",
      "  549 944 779 207 648 263 608 289 751 692  87 502 525  28 164 555 176 566\n",
      "  362 806 713 827 493 223 690 578 894 695 361  99 558 865 711 487 763 789\n",
      "  152 135 567 497 409 544 295 750 994 914 129 863 575 995 524   6  80 471\n",
      "  256 590 228 451 244 905 911 881 594 411 954 419 396 739 604 178 469 801\n",
      "  856 793  21 528 437 448 340 940 146 171 748 452 455 309 432 634 716 149\n",
      "  922 965 297 479 969 930 951 241 257 336 311 516  35 533 953 671  51 365\n",
      "  764 386 794  15  47 303 192 430 620 363 705 120 344 675 780 693 422 371\n",
      "  610 950 574 306 299 626 445 823 982 680 607 481 224 970 559 450 798 660\n",
      "  611 531 638 291 200  24  48 536 325 225 199 891 294 310 270 520 462  74\n",
      "   38 205 197 206 658 190 109 449 509 489 198 260 203 480 734 147 463 119\n",
      "  521 240  16 664 519 394 766 715 391 441 627  31 116 956 993 924 318 636\n",
      "  754 991  59 335 717 732 762 328 812 834 820 948 878 474 387 613 343 486\n",
      "  510 786 209 395 177 761  32 698 699 898 915 142 478 128 665 100  58 235\n",
      "  179 569 857 266 600 588 833 354  91  88 145 959 937 563 877 435 797 649\n",
      "  858  44 740 132 708 117 399 406 741 364 859 895 339 874 612 632 702 527\n",
      "  913 101 429  89 418 259 175 137 967 919 397 584 496 443 341 752 921  30\n",
      "  576 505 456 370 968 889 547 508 102  85 879 980 220 587 822 150 247  76\n",
      "  538 656 978 595 252 208 402 383 622 189 854 903 350 654 180 321 304 955\n",
      "   84 829 168  29 725 639 855 726 679 724]]\n",
      "[[499 579 580 581 582 583 585 586 591 577 592 597 598 599 601 602 603 605\n",
      "  606 593 573 572 570 530 534 537 539 540 541 542 543 545 546 550 551 552\n",
      "  553 556 561 562 564 565 609 614 615 616 653 655 657 661 662 663 666 667\n",
      "  668 669 670 672 673 674 676 677 678 681 682 652 529 651 647 617 619 621\n",
      "  623 624 625 628 629 630 631 633 635 637 640 641 642 643 644 645 650 526\n",
      "  523 522 405 407 410 412 413 414 415 416 417 420 423 424 425 426 427 428\n",
      "  431 433 434 404 436 403 400 369 373 374 375 376 377 378 379 380 381 382\n",
      "  384 385 388 389 390 392 393 398 401 683 438 440 490 491 492 494 495 498\n",
      "  998 500 503 504 506 507 511 512 513 514 515 517 518 488 439 484 482 444\n",
      "  446 447 453 457 458 459 460 461 465 466 467 468 470 472 473 475 476 477\n",
      "  483 684 685 687 876 880 882 883 884 885 887 888 890 892 893 896 897 899\n",
      "  902 904 906 907 908 875 909 873 871 841 842 843 844 847 848 849 850 851\n",
      "  852 853 860 861 862 864 866 868 869 870 872 840 912 918 964 966 971 972\n",
      "  973 974 975 976 977 979 981 983 985 986 987 988 990 996 997 963 917 961\n",
      "  958 923 925 926 927 928 929 931 932 934 936 938 939 941 945 946 947 949\n",
      "  952 957 960 367 838 836 729 730 731 733 735 736 737 738 742 744 745 746\n",
      "  747 749 753 755 756 757 758 728 759 727 722 688 689 691 694 696 697 700\n",
      "  701 703 704 706 707 709 710 712 714 719 720 721 723 837 760 768 800 802\n",
      "  803 804 805 809 811 813 816 817 819 821 824 825 826 828 830 831 835 799\n",
      "  767 796 792 769 770 771 772 773 774 775 776 777 778 781 782 783 784 785\n",
      "  787 788 790 791 795 366 999 184 264 105 262 106 261 107 258 108 110 111\n",
      "  112 113 255 114 253 115 118 122 251 250 123 124 249 126 248 127 246 265\n",
      "  245 104 268  78 290  79  81  82 288 287  83 286 285  86 282 281 280 279\n",
      "  195  90 277  93 274  94 273  96  97 271  98 103 267 130 131 243 162 163\n",
      "  219 218 217 165 216 166 215 167 214 169 212 211 210 172 173 182 183 204\n",
      "  185 186 187 201 188 193 194 221 222 161 160 242 133 136 138 239 139 140\n",
      "  237 236 141 143 234 233  77 144 231 148 230 151 153 154 155 229 156 227\n",
      "  226 158 159 232 292 278  75 337  27 334 333 332 331 330 329  34 326  36\n",
      "   37 324  39 323 293  40  41  42 320 319  43 317  45 316 315  46  26  49\n",
      "   25  22   2   3 359 358 357 356 355   4   5 352 351   7   8 349 348   9\n",
      "  347  11  12  13 345  14  17  18  19 342  20  23  50 322 196 296  70 298\n",
      "  308 307 305  60  61  72  62 302  64 301 300  65  69  68  66  63  54  67\n",
      "  312  53  73 313  52   1 501  95 338 327 121 992 807 254 571  92  10 314\n",
      "  743 202 213 353 765 368 984 900 942 157 916 421 532 284 810 814 901 846\n",
      "  269 283 560  57 933 845 174 618 596 962 910 589 839 646 442 408 989 170\n",
      "  372 276 125 454 464  71 554 818 557 659 346 485 815 535  33 238 718 832\n",
      "  920 886 568  56 867 360 935 134 686 808 181 272 548 191 943 275  55   0\n",
      "  549 944 779 207 648 263 608 289 751 692  87 502 525  28 164 555 176 566\n",
      "  362 806 713 827 493 223 690 578 894 695 361  99 558 865 711 487 763 789\n",
      "  152 135 567 497 409 544 295 750 994 914 129 863 575 995 524   6  80 471\n",
      "  256 590 228 451 244 905 911 881 594 411 954 419 396 739 604 178 469 801\n",
      "  856 793  21 528 437 448 340 940 146 171 748 452 455 309 432 634 716 149\n",
      "  922 965 297 479 969 930 951 241 257 336 311 516  35 533 953 671  51 365\n",
      "  764 386 794  15  47 303 192 430 620 363 705 120 344 675 780 693 422 371\n",
      "  610 950 574 306 299 626 445 823 982 680 607 481 224 970 559 450 798 660\n",
      "  611 531 638 291 200  24  48 536 325 225 199 891 294 310 270 520 462  74\n",
      "   38 205 197 206 658 190 109 449 509 489 198 260 203 480 734 147 463 119\n",
      "  521 240  16 664 519 394 766 715 391 441 627  31 116 956 993 924 318 636\n",
      "  754 991  59 335 717 732 762 328 812 834 820 948 878 474 387 613 343 486\n",
      "  510 786 209 395 177 761  32 698 699 898 915 142 478 128 665 100  58 235\n",
      "  179 569 857 266 600 588 833 354  91  88 145 959 937 563 877 435 797 649\n",
      "  858  44 740 132 708 117 399 406 741 364 859 895 339 874 612 632 702 527\n",
      "  913 101 429  89 418 259 175 137 967 919 397 584 496 443 341 752 921  30\n",
      "  576 505 456 370 968 889 547 508 102  85 879 980 220 587 822 150 247  76\n",
      "  538 656 978 595 252 208 402 383 622 189 854 903 350 654 180 321 304 955\n",
      "   84 829 168  29 725 639 855 726 679 724]]\n",
      "[[0.005 0.007 0.003 0.003 0.006 0.004 0.006 0.005 0.002 0.008 0.004 0.003\n",
      "  0.004 0.002 0.003 0.002 0.01  0.003 0.003 0.002 0.006 0.003 0.003 0.003\n",
      "  0.011 0.003 0.003 0.002 0.024 0.016 0.003 0.006 0.02  0.008 0.008 0.003\n",
      "  0.004 0.003 0.014 0.002 0.004 0.009 0.005 0.005 0.03  0.003 0.003 0.003\n",
      "  0.009 0.003 0.003 0.003 0.01  0.004 0.004 0.006 0.005 0.013 0.012 0.013\n",
      "  0.008 0.003 0.004 0.003 0.003 0.005 0.003 0.004 0.003 0.003 0.003 0.004\n",
      "  0.003 0.003 0.004 0.002 0.014 0.002 0.005 0.004 0.01  0.003 0.003 0.003\n",
      "  0.011 0.015 0.016 0.006 0.002 0.033 0.003 0.006 0.021 0.004 0.003 0.021\n",
      "  0.002 0.017 0.003 0.003 0.002 0.02  0.024 0.004 0.002 0.002 0.006 0.004\n",
      "  0.004 0.027 0.004 0.003 0.003 0.003 0.003 0.006 0.01  0.011 0.005 0.016\n",
      "  0.004 0.005 0.002 0.002 0.004 0.009 0.003 0.002 0.028 0.003 0.008 0.003\n",
      "  0.009 0.008 0.004 0.002 0.008 0.004 0.004 0.003 0.002 0.002 0.003 0.003\n",
      "  0.005 0.028 0.009 0.003 0.003 0.008 0.029 0.003 0.002 0.003 0.002 0.007\n",
      "  0.005 0.002 0.014 0.003 0.002 0.007 0.009 0.003 0.004 0.025 0.006 0.002\n",
      "  0.003 0.003 0.022 0.004 0.006 0.002 0.011 0.016 0.002 0.003 0.003 0.01\n",
      "  0.005 0.003 0.002 0.006 0.002 0.028 0.002 0.004 0.003 0.027 0.003 0.004\n",
      "  0.004 0.004 0.007 0.003 0.004 0.002 0.008 0.008 0.007 0.002 0.003 0.011\n",
      "  0.004 0.003 0.003 0.003 0.011 0.006 0.003 0.005 0.005 0.003 0.003 0.002\n",
      "  0.003 0.003 0.004 0.002 0.018 0.003 0.002 0.003 0.014 0.003 0.002 0.003\n",
      "  0.032 0.004 0.002 0.009 0.004 0.007 0.004 0.003 0.003 0.003 0.006 0.002\n",
      "  0.018 0.003 0.003 0.003 0.014 0.004 0.003 0.01  0.006 0.004 0.002 0.003\n",
      "  0.006 0.005 0.005 0.004 0.005 0.008 0.009 0.002 0.003 0.004 0.003 0.009\n",
      "  0.002 0.003 0.002 0.003 0.003 0.019 0.016 0.003 0.017 0.003 0.003 0.011\n",
      "  0.022 0.007 0.002 0.003 0.004 0.002 0.012 0.007 0.013 0.007 0.004 0.003\n",
      "  0.005 0.002 0.002 0.01  0.002 0.004 0.005 0.007 0.005 0.002 0.002 0.004\n",
      "  0.004 0.004 0.004 0.003 0.001 0.003 0.003 0.003 0.006 0.006 0.008 0.005\n",
      "  0.003 0.003 0.004 0.002 0.006 0.002 0.007 0.003 0.011 0.004 0.004 0.004\n",
      "  0.004 0.006 0.004 0.005 0.012 0.003 0.005 0.003 0.003 0.004 0.003 0.002\n",
      "  0.002 0.002 0.006 0.013 0.005 0.002 0.006 0.028 0.002 0.004 0.006 0.003\n",
      "  0.003 0.002 0.033 0.003 0.002 0.003 0.02  0.006 0.003 0.015 0.002 0.004\n",
      "  0.006 0.003 0.01  0.007 0.006 0.004 0.003 0.006 0.012 0.003 0.006 0.002\n",
      "  0.02  0.002 0.004 0.009 0.013 0.003 0.006 0.006 0.003 0.005 0.004 0.002\n",
      "  0.002 0.004 0.004 0.027 0.002 0.004 0.003 0.002 0.002 0.002 0.009 0.01\n",
      "  0.009 0.017 0.005 0.004 0.004 0.002 0.038 0.005 0.005 0.005 0.003 0.002\n",
      "  0.007 0.008 0.007 0.007 0.002 0.014 0.003 0.005 0.01  0.003 0.024 0.004\n",
      "  0.003 0.003 0.006 0.003 0.006 0.003 0.003 0.003 0.003 0.012 0.002 0.006\n",
      "  0.003 0.004 0.002 0.004 0.002 0.004 0.006 0.003 0.005 0.002 0.006 0.004\n",
      "  0.003 0.011 0.004 0.004 0.007 0.004 0.011 0.002 0.004 0.003 0.005 0.014\n",
      "  0.012 0.006 0.003 0.004 0.002 0.012 0.029 0.003 0.004 0.004 0.003 0.003\n",
      "  0.004 0.005 0.014 0.039 0.004 0.004 0.005 0.003 0.007 0.002 0.003 0.003\n",
      "  0.006 0.004 0.003 0.007 0.004 0.002 0.004 0.002 0.004 0.011 0.015 0.003\n",
      "  0.002 0.008 0.003 0.004 0.027 0.002 0.002 0.004 0.008 0.007 0.005 0.015\n",
      "  0.003 0.024 0.004 0.004 0.006 0.002 0.01  0.005 0.005 0.002 0.003 0.005\n",
      "  0.002 0.002 0.003 0.013 0.006 0.008 0.011 0.003 0.005 0.005 0.004 0.043\n",
      "  0.005 0.004 0.003 0.003 0.003 0.005 0.002 0.004 0.005 0.007 0.035 0.002\n",
      "  0.003 0.003 0.003 0.003 0.006 0.004 0.003 0.029 0.005 0.002 0.003 0.006\n",
      "  0.003 0.003 0.009 0.004 0.009 0.008 0.004 0.002 0.012 0.014 0.007 0.009\n",
      "  0.003 0.003 0.008 0.01  0.004 0.038 0.003 0.005 0.002 0.004 0.004 0.005\n",
      "  0.021 0.003 0.017 0.003 0.002 0.005 0.002 0.003 0.01  0.002 0.002 0.029\n",
      "  0.015 0.01  0.006 0.003 0.003 0.003 0.006 0.017 0.005 0.006 0.003 0.004\n",
      "  0.005 0.002 0.002 0.003 0.005 0.004 0.004 0.009 0.004 0.003 0.007 0.012\n",
      "  0.025 0.004 0.007 0.004 0.003 0.003 0.005 0.003 0.003 0.006 0.044 0.004\n",
      "  0.003 0.002 0.002 0.008 0.006 0.003 0.003 0.002 0.022 0.002 0.006 0.002\n",
      "  0.023 0.007 0.002 0.003 0.003 0.002 0.005 0.011 0.002 0.005 0.003 0.003\n",
      "  0.004 0.007 0.004 0.004 0.004 0.003 0.021 0.003 0.005 0.003 0.018 0.005\n",
      "  0.003 0.004 0.007 0.005 0.003 0.013 0.013 0.005 0.002 0.004 0.003 0.002\n",
      "  0.004 0.004 0.005 0.007 0.002 0.005 0.003 0.001 0.004 0.016 0.003 0.004\n",
      "  0.007 0.002 0.012 0.009 0.003 0.003 0.002 0.007 0.002 0.008 0.003 0.003\n",
      "  0.008 0.002 0.01  0.023 0.002 0.004 0.025 0.006 0.002 0.006 0.004 0.004\n",
      "  0.003 0.002 0.004 0.008 0.008 0.02  0.006 0.003 0.007 0.002 0.007 0.002\n",
      "  0.003 0.003 0.004 0.002 0.004 0.    0.    0.003 0.002 0.005 0.006 0.003\n",
      "  0.013 0.002 0.02  0.003 0.007 0.003 0.002 0.006 0.024 0.003 0.003 0.004\n",
      "  0.012 0.002 0.006 0.002 0.01  0.004 0.003 0.002 0.002 0.005 0.003 0.002\n",
      "  0.002 0.002 0.002 0.005 0.004 0.014 0.003 0.008 0.003 0.013 0.01  0.008\n",
      "  0.003 0.004 0.003 0.007 0.021 0.003 0.008 0.006 0.007 0.003 0.003 0.002\n",
      "  0.002 0.004 0.006 0.004 0.002 0.005 0.008 0.003 0.006 0.003 0.003 0.003\n",
      "  0.003 0.004 0.002 0.002 0.007 0.009 0.005 0.005 0.002 0.004 0.003 0.004\n",
      "  0.003 0.003 0.003 0.005 0.006 0.004 0.002 0.003 0.008 0.004 0.002 0.002\n",
      "  0.003 0.003 0.003 0.004 0.019 0.003 0.02  0.003 0.004 0.003 0.005 0.011\n",
      "  0.003 0.026 0.003 0.004 0.003 0.014 0.004 0.006 0.003 0.004 0.002 0.022\n",
      "  0.003 0.003 0.004 0.004 0.006 0.011 0.008 0.003 0.005 0.015 0.002 0.003\n",
      "  0.003 0.005 0.011 0.009 0.004 0.012 0.003 0.002 0.004 0.004 0.002 0.006\n",
      "  0.003 0.004 0.002 0.011 0.003 0.003 0.01  0.002 0.003 0.006 0.015 0.003\n",
      "  0.007 0.018 0.009 0.014 0.003 0.004 0.004 0.005 0.009 0.003 0.003 0.005\n",
      "  0.002 0.04  0.006 0.006 0.004 0.005 0.003 0.021 0.004 0.008 0.003 0.003\n",
      "  0.013 0.011 0.003 0.009 0.012 0.005 0.003 0.003 0.003 0.003 0.005 0.012\n",
      "  0.004 0.007 0.01  0.003 0.002 0.004 0.009 0.004 0.004 0.012 0.006 0.002\n",
      "  0.01  0.003 0.003 0.003 0.009 0.003 0.005 0.011 0.003 0.01  0.003 0.003\n",
      "  0.003 0.014 0.003 0.002 0.005 0.005 0.003 0.01  0.005 0.005 0.003 0.004\n",
      "  0.005 0.003 0.004 0.002 0.004 0.003 0.004 0.013 0.013 0.006 0.003 0.006\n",
      "  0.002 0.004 0.005 0.006 0.003 0.007 0.005 0.007 0.012 0.005 0.003 0.002\n",
      "  0.003 0.002 0.005 0.013 0.003 0.003 0.027 0.003 0.023 0.003 0.006 0.003\n",
      "  0.003 0.004 0.007 0.003 0.003 0.006 0.002 0.003 0.005 0.005 0.003 0.014\n",
      "  0.006 0.012 0.004 0.003]]\n",
      "[[725 726 679 304 289 717 888 585 259  15 516 226  27 290 167  96 738 723\n",
      "  671 814 719 559 477 157 391 513   8 441 336 317 690 315 383 810 586 123\n",
      "  349 281 460 407 751 297 779 871 239 100 344 638 685 436 278 264 973  75\n",
      "  219 182 184 859 122 633 709 222 704 173 509 549 635 733 154 728 641 601\n",
      "  534 497 757 298 960 141 485 758 160 412 539 517 752 700 916 434  19 352\n",
      "  201 266 250 697 862 572  88  39 373 756 794 676 990 371 215 951 668 815\n",
      "  384 230 582 971 923 644 127 337 850 341 197 939 784 392 692 487 388 176\n",
      "  755 152 580 631 602 104 800 393 492  13 292 838 140 795 135 745 335 498\n",
      "  430 401 780 626 186  77 625 747 451 105 358 866  68  63 721 305 151 523\n",
      "   72 369 153 532 977 768 552 731 949 334   3 791  83 478 423  11 181 541\n",
      "  577 421 754 688 689 764  46 329 640 972 550 593 864 811 163  51 737 241\n",
      "  479 836 271 938 935 366  81  94 907  18  61 351 830 802 206 880 792 205\n",
      "  816 981 491 946 932 902 591 427 303  45 620 273  49 530 790 926 246 570\n",
      "  852 444 976 428 458 624 647  17 806 720 583 467  69  26  70 885 243 750\n",
      "  274 265 821 936 426 964 929  66 331  82 872 227 225  14 970 875 159 991\n",
      "  380  22 598 147  90 823 715 542 514 439 908 262 741 657 216 805 579 482\n",
      "  988 332 629 287 169 983 773 817 825 142 994 630 312 356 207 987 139 592\n",
      "  268 260 999 114 942 313 655 129 319 639 958 126 504 518 221 840 869 214\n",
      "  984 377 143  21  64 112  73 425  98 832 899  25 190 307 546 475 841 603\n",
      "   35  47 682 463 178 617 540 727 417 660 347 432 925 818 543 531 787  23\n",
      "  619 553 979 616 953  30 148 609 251 235 847 777 927 242 909 778 217 898\n",
      "  858 906 851 453 168 414 279 708 915 494 695 210 886 237 828 646 306 111\n",
      "  934 466 664 420  50 213  37  99 131 653 565 223 361   2 390 735 670 742\n",
      "  195 770 113 762 236 678 894 789 267 188 353 868 564 694 202 406 804 348\n",
      "  177 245 322 137 389 124 623 599 710 108 486 187 954 892 558 809 760 443\n",
      "  301 769 706 606 865 843 484 831 961 374  12 882 499 488 382 468 280 452\n",
      "  837 526 669 495 447 615 919 286 321 680 803 535 192 134 255 300 917 952\n",
      "  605 419 568 386 345  74 813 998 947 819 860 365 229 326 138 193 896 302\n",
      "  950 218 920 234 261 801 661 608 648 399 107 613 164 701  53  10 856 722\n",
      "  574 783 481 110 191 743 506 103 673 433 232 465 683 749 529  36 120 724\n",
      "  573 400  93 437 459  40 449 314 834  62 793 650  54 299 324 204 171  67\n",
      "   79 707 472 672   5 323 196 385 435 545 652 881 507 446 861 781 359 555\n",
      "  473 333  71 249 912 842 293 464 824 985 651 807 663 525 674 474 905 515\n",
      "  528 256 211 405 667 759 469 785 294 992 941 962 853 415 966  42 969 940\n",
      "  156 799 883 440  43 645 536 212 571 798 618 945 524 381 677 974 330 511\n",
      "  893 753 454 144 311 340 729 533 575 642 596 581 398 604 327 656 118 502\n",
      "  659  65 826 910 993 404 121 403 848 296 180 548   0  56   7 930 948 288\n",
      "  254 253 600 944 512 887  78 342 364 166 863 422 590 782 788 480 442 890\n",
      "  957 370  91 183 544 922 457 963 594 379  55 551 355  31 959 508 209 172\n",
      "  730 367 746 597 308 252 634 705 891 775 835 338 378 739 873 360 115 325\n",
      "   20  87 309 982 438 346   6 714 248 621 431 520 106 424 844 808 703   4\n",
      "  238 316 996 989 628 649 718 233 614 637 194 408 967 283 200 965 501 537\n",
      "  448 913 363 318 410 610   1 662 876 796 986 736 562 155 776 684 771 277\n",
      "  285 483 161 295 691 675 476 716 411 693 149 712 133 521 767 897 763 130\n",
      "   34 774 493 557 696   9  33 136 310 409 199 627 257 812 198 500 786  60\n",
      "  846 566 711 855  41  48 231 132 375 928 258 554 396 162 878 884 903 125\n",
      "  797 263 563 394 556 146 687 607 918 416 870 698 924 116 914 179  80 766\n",
      "   52 943 395 247 567 748 589 584 933 510  16 362 291 174  24 854 845  84\n",
      "  643 827 117 445 901 208 450 275 931 867 203 320 522 489 921 911 611 461\n",
      "  368 328  58 456 282 686 968 997 857 429 744 904 560 956 666  59  57 975\n",
      "  900 284 732 339 665 955 376 765 519 158 413 561 879 224 995 244 937 470\n",
      "   38 833  76 455 761 874 588 503  85 357 849 490  29 681 175  86 270 119\n",
      "  397 595 578  97 272 240 658 220 877 820 269 734 101 354 372 713  32 822\n",
      "   92 576 772 895 654  95 632 276 839 170 980 636 699 740 418 505  28 102\n",
      "  612 165 702 829 387 109 496 978 189 185 145 343 128 462 150 587 547  44\n",
      "  228 350  89 538 569 402 471 889 527 622]]\n",
      "[[725 726 679 304 289 717 888 585 259  15 516 226  27 290 167  96 738 723\n",
      "  671 814 719 559 477 157 391 513   8 441 336 317 690 315 383 810 586 123\n",
      "  349 281 460 407 751 297 779 871 239 100 344 638 685 436 278 264 973  75\n",
      "  219 182 184 859 122 633 709 222 704 173 509 549 635 733 154 728 641 601\n",
      "  534 497 757 298 960 141 485 758 160 412 539 517 752 700 916 434  19 352\n",
      "  201 266 250 697 862 572  88  39 373 756 794 676 990 371 215 951 668 815\n",
      "  384 230 582 971 923 644 127 337 850 341 197 939 784 392 692 487 388 176\n",
      "  755 152 580 631 602 104 800 393 492  13 292 838 140 795 135 745 335 498\n",
      "  430 401 780 626 186  77 625 747 451 105 358 866  68  63 721 305 151 523\n",
      "   72 369 153 532 977 768 552 731 949 334   3 791  83 478 423  11 181 541\n",
      "  577 421 754 688 689 764  46 329 640 972 550 593 864 811 163  51 737 241\n",
      "  479 836 271 938 935 366  81  94 907  18  61 351 830 802 206 880 792 205\n",
      "  816 981 491 946 932 902 591 427 303  45 620 273  49 530 790 926 246 570\n",
      "  852 444 976 428 458 624 647  17 806 720 583 467  69  26  70 885 243 750\n",
      "  274 265 821 936 426 964 929  66 331  82 872 227 225  14 970 875 159 991\n",
      "  380  22 598 147  90 823 715 542 514 439 908 262 741 657 216 805 579 482\n",
      "  988 332 629 287 169 983 773 817 825 142 994 630 312 356 207 987 139 592\n",
      "  268 260 999 114 942 313 655 129 319 639 958 126 504 518 221 840 869 214\n",
      "  984 377 143  21  64 112  73 425  98 832 899  25 190 307 546 475 841 603\n",
      "   35  47 682 463 178 617 540 727 417 660 347 432 925 818 543 531 787  23\n",
      "  619 553 979 616 953  30 148 609 251 235 847 777 927 242 909 778 217 898\n",
      "  858 906 851 453 168 414 279 708 915 494 695 210 886 237 828 646 306 111\n",
      "  934 466 664 420  50 213  37  99 131 653 565 223 361   2 390 735 670 742\n",
      "  195 770 113 762 236 678 894 789 267 188 353 868 564 694 202 406 804 348\n",
      "  177 245 322 137 389 124 623 599 710 108 486 187 954 892 558 809 760 443\n",
      "  301 769 706 606 865 843 484 831 961 374  12 882 499 488 382 468 280 452\n",
      "  837 526 669 495 447 615 919 286 321 680 803 535 192 134 255 300 917 952\n",
      "  605 419 568 386 345  74 813 998 947 819 860 365 229 326 138 193 896 302\n",
      "  950 218 920 234 261 801 661 608 648 399 107 613 164 701  53  10 856 722\n",
      "  574 783 481 110 191 743 506 103 673 433 232 465 683 749 529  36 120 724\n",
      "  573 400  93 437 459  40 449 314 834  62 793 650  54 299 324 204 171  67\n",
      "   79 707 472 672   5 323 196 385 435 545 652 881 507 446 861 781 359 555\n",
      "  473 333  71 249 912 842 293 464 824 985 651 807 663 525 674 474 905 515\n",
      "  528 256 211 405 667 759 469 785 294 992 941 962 853 415 966  42 969 940\n",
      "  156 799 883 440  43 645 536 212 571 798 618 945 524 381 677 974 330 511\n",
      "  893 753 454 144 311 340 729 533 575 642 596 581 398 604 327 656 118 502\n",
      "  659  65 826 910 993 404 121 403 848 296 180 548   0  56   7 930 948 288\n",
      "  254 253 600 944 512 887  78 342 364 166 863 422 590 782 788 480 442 890\n",
      "  957 370  91 183 544 922 457 963 594 379  55 551 355  31 959 508 209 172\n",
      "  730 367 746 597 308 252 634 705 891 775 835 338 378 739 873 360 115 325\n",
      "   20  87 309 982 438 346   6 714 248 621 431 520 106 424 844 808 703   4\n",
      "  238 316 996 989 628 649 718 233 614 637 194 408 967 283 200 965 501 537\n",
      "  448 913 363 318 410 610   1 662 876 796 986 736 562 155 776 684 771 277\n",
      "  285 483 161 295 691 675 476 716 411 693 149 712 133 521 767 897 763 130\n",
      "   34 774 493 557 696   9  33 136 310 409 199 627 257 812 198 500 786  60\n",
      "  846 566 711 855  41  48 231 132 375 928 258 554 396 162 878 884 903 125\n",
      "  797 263 563 394 556 146 687 607 918 416 870 698 924 116 914 179  80 766\n",
      "   52 943 395 247 567 748 589 584 933 510  16 362 291 174  24 854 845  84\n",
      "  643 827 117 445 901 208 450 275 931 867 203 320 522 489 921 911 611 461\n",
      "  368 328  58 456 282 686 968 997 857 429 744 904 560 956 666  59  57 975\n",
      "  900 284 732 339 665 955 376 765 519 158 413 561 879 224 995 244 937 470\n",
      "   38 833  76 455 761 874 588 503  85 357 849 490  29 681 175  86 270 119\n",
      "  397 595 578  97 272 240 658 220 877 820 269 734 101 354 372 713  32 822\n",
      "   92 576 772 895 654  95 632 276 839 170 980 636 699 740 418 505  28 102\n",
      "  612 165 702 829 387 109 496 978 189 185 145 343 128 462 150 587 547  44\n",
      "  228 350  89 538 569 402 471 889 527 622]]\n"
     ]
    }
   ],
   "source": [
    "# compute_centroid:  given X, a review-term matrix of sparse vectors generated by\n",
    "# a vectorizer, and a list of row indices to extract from that matrix,\n",
    "# return the centroid of the selected vectors.\n",
    "def compute_centroid2(X, node_list):\n",
    "    cluster_instances = X[node_list, :]\n",
    "    dense_cluster_instances = cluster_instances.todense()\n",
    "    centroid = np.mean(dense_cluster_instances, axis = 0)\n",
    "    return centroid\n",
    "\n",
    "results = []\n",
    "#Step 4\n",
    "for i in range(clusters.max()):\n",
    "    filtered = np.where(clusters == i+1)\n",
    "    centroid = compute_centroid2(X, filtered[0])\n",
    "    print(centroid)\n",
    "    centroid = np.argsort(centroid)\n",
    "    print(centroid)\n",
    "    centroid = centroid[::-1]\n",
    "    print(centroid)\n",
    "    results.append(centroid)\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: [pumpkin] [pie] [pumpkin pie] [spices] [nutmeg]\n",
      "Cluster 1: [nice] [light] [sweet] [hops] [good]\n"
     ]
    }
   ],
   "source": [
    "names = [[724, 679, 726, 855, 639, 725],\n",
    "        [622, 527, 889, 471, 402, 569]]\n",
    "\n",
    "names = np.array(names)\n",
    "\n",
    "print_cluster_features(vectorizer, names, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "056d184510d307feb9e391e7e9b79bf7",
     "grade": false,
     "grade_id": "cell-cb76c12ea7536548",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pumpkin', 'pie', 'pumpkin pie', 'spices', 'nutmeg'],\n",
       " ['nice', 'light', 'sweet', 'hops', 'good']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "def answer_hierarchy():\n",
    "    result = [['pumpkin', 'pie', 'pumpkin pie', 'spices', 'nutmeg'], \n",
    "              ['nice', 'light', 'sweet', 'hops', 'good']]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return result\n",
    "\n",
    "answer_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "111cccde5038b01a8264cdb786214a36",
     "grade": true,
     "grade_id": "cell-09d1290a9c6bb5d6",
     "locked": true,
     "points": 25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_hierarchy()\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q3: Your function should return a list (of string lists). \"\n",
    "assert np.array([type(elt) == list for elt in stu_ans]).all(), \"Q3: each cluster summary should be a list (of terms).\"\n",
    "assert np.array([len(elt) == 5 for elt in stu_ans]).all(), \"Q3: each cluster summary should have exactly 5 terms.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4. Clustering with mixed feature types and cluster representatives. (25 points) \n",
    "\n",
    "The classic version of k-means assumes squared Euclidean distance. This assumption is used when the cluster representatives $m_k$ are assumed to be the means of the currently assigned cluster members. This is fine if all the features/variables are continuous values where taking the mean over them makes sense. However, there are many situations where we have a mixture of feature types, including categorical or ordinal features, where computing a mean doesn't make sense. In these cases, we must consider a more general version of k-means, based on an arbitrary dissimilarity function $D(x_i, x_j)$ between items that is used to compute the optimal cluster representatives $m_k$ (instead of a mean). One widely-used example of this is *k-medoids clustering*, which finds an existing item in the cluster that has minimum total distance to the other points in the cluster. (Even more generally, we can define a clustering algorithm just using proximity matrices of items, if they are available, and not even computing cluster centers: just keeping the item index of the cluster representative.)\n",
    "\n",
    "We're going use k-medoids for this question - but scikit-learn doesn't yet have a k-medoids implementation, so we're going to use a package called sklearn-extra, which is installed for you in the unsupervised learning Coursera environment. Note that k-medoid is computationally expensive: a naive implementation is quadratic in its runtime requirements (in $n$, the number of data instances) although more clever implementations can reduce that.\n",
    "\n",
    "Another problem this question deals with is how to do clustering when your data has a mix of feature types. We solve this here by computing a generalized form of distance between instances called the *Gower distance.*\n",
    "\n",
    "Here's a summary of how Gower distance is computed:  For each variable type, pick a distance that scales 0 to 1, then take a linear combination of these distance based on user-specified weights.\n",
    " \n",
    "For quantative (interval) variables: use manhattan distance.\n",
    "For ordinal variables: rank or manhattan distances, with special adjustment for ties.\n",
    "For nominal variables: variables of k categories are converted to one-hot encoding (k binary columns) and compared using the Dice coefficient.\n",
    "\n",
    " \n",
    "For more details about the Gower distance, see: Gower, J.C., 1971, A General Coefficient of Similarity and Some of Its Properties. Biometrics.  Vol. 27, No. 4 (Dec., 1971), pp. 857-871. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're going to work with the following dataset about U.S. colleges and universities. It contains statistics for a large number of US Colleges from the 1995 issue of US News and World Report.\n",
    "\n",
    "A data frame with 777 observations on the following 18 variables:\n",
    "\n",
    "`Private` A factor with levels No and Yes indicating private or public university\n",
    "\n",
    "`Apps` Number of applications received\n",
    "\n",
    "`Accept` Number of applications accepted\n",
    "\n",
    "`Enroll` Number of new students enrolled\n",
    "\n",
    "`Top10perc` Pct. new students from top 10% of H.S. class\n",
    "\n",
    "`Top25perc` Pct. new students from top 25% of H.S. class\n",
    "\n",
    "`F.Undergrad` Number of fulltime undergraduates\n",
    "\n",
    "`P.Undergrad` Number of parttime undergraduates\n",
    "\n",
    "`Outstate` Out-of-state tuition\n",
    "\n",
    "`Room.Board` Room and board costs\n",
    "\n",
    "`Books` Estimated book costs\n",
    "\n",
    "`Personal` Estimated personal spending\n",
    "\n",
    "`PhD` Pct. of faculty with Ph.D.s\n",
    "\n",
    "`Terminal` Pct. of faculty with terminal degree\n",
    "\n",
    "`S.F.Ratio` Student/faculty ratio\n",
    "\n",
    "`perc.alumni` Pct. alumni who donate\n",
    "\n",
    "`Expend` Instructional expenditure per student\n",
    "\n",
    "`Grad.Rate` Graduation rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./assets/college.csv')\n",
    "df = df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>College.Name</th>\n",
       "      <th>Private</th>\n",
       "      <th>Apps</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene Christian University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1660</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelphi University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2186</td>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adrian College</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1428</td>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agnes Scott College</td>\n",
       "      <td>Yes</td>\n",
       "      <td>417</td>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alaska Pacific University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>193</td>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   College.Name Private  Apps  Accept  Enroll  Top10perc  \\\n",
       "0  Abilene Christian University     Yes  1660    1232     721         23   \n",
       "1            Adelphi University     Yes  2186    1924     512         16   \n",
       "2                Adrian College     Yes  1428    1097     336         22   \n",
       "3           Agnes Scott College     Yes   417     349     137         60   \n",
       "4     Alaska Pacific University     Yes   193     146      55         16   \n",
       "\n",
       "   Top25perc  F.Undergrad  P.Undergrad  Outstate  Room.Board  Books  Personal  \\\n",
       "0         52         2885          537      7440        3300    450      2200   \n",
       "1         29         2683         1227     12280        6450    750      1500   \n",
       "2         50         1036           99     11250        3750    400      1165   \n",
       "3         89          510           63     12960        5450    450       875   \n",
       "4         44          249          869      7560        4120    800      1500   \n",
       "\n",
       "   PhD  Terminal  S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n",
       "0   70        78       18.1           12    7041         60  \n",
       "1   29        30       12.2           16   10527         56  \n",
       "2   53        66       12.9           30    8735         54  \n",
       "3   92        97        7.7           37   19016         59  \n",
       "4   76        72       11.9            2   10922         15  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can use the provided gower_distances function below to compute a Gower distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance \n",
    "from sklearn.utils import validation\n",
    "from sklearn.metrics import pairwise\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# The following functions are provided by Marcelo Beckmann and currently are part of the scikit-learn\n",
    "# github repository but have not yet been officially released.\n",
    "\n",
    "# This is a utility function used by gower_distances below.\n",
    "def check_pairwise_arrays(X, Y, precomputed=False, dtype=None):\n",
    "    X, Y, dtype_float = pairwise._return_float_dtype(X, Y)\n",
    "\n",
    "    warn_on_dtype = dtype is not None\n",
    "    estimator = 'check_pairwise_arrays'\n",
    "    if dtype is None:\n",
    "        dtype = dtype_float\n",
    "\n",
    "    if Y is X or Y is None:\n",
    "        X = Y = validation.check_array(X, accept_sparse='csr', dtype=dtype,\n",
    "                            estimator=estimator)\n",
    "    else:\n",
    "        X = validation.check_array(X, accept_sparse='csr', dtype=dtype,\n",
    "                        estimator=estimator)\n",
    "        Y = validation.check_array(Y, accept_sparse='csr', dtype=dtype,\n",
    "                        estimator=estimator)\n",
    "\n",
    "    if precomputed:\n",
    "        if X.shape[1] != Y.shape[0]:\n",
    "            raise ValueError(\"Precomputed metric requires shape \"\n",
    "                             \"(n_queries, n_indexed). Got (%d, %d) \"\n",
    "                             \"for %d indexed.\" %\n",
    "                             (X.shape[0], X.shape[1], Y.shape[0]))\n",
    "    elif X.shape[1] != Y.shape[1]:\n",
    "        raise ValueError(\"Incompatible dimension for X and Y matrices: \"\n",
    "                         \"X.shape[1] == %d while Y.shape[1] == %d\" % (\n",
    "                             X.shape[1], Y.shape[1]))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# gower_distances: see header below for details.\n",
    "#\n",
    "# For purposes of this assignment you can simply call it like this on a dataset X (n instances):\n",
    "# D = gower_distances(X)\n",
    "#\n",
    "# which results an n x n distance matrix.\n",
    "#\n",
    "# Source: M. Beckmann  https://github.com/scikit-learn/scikit-learn/pull/9555 \n",
    "\n",
    "\n",
    "def gower_distances(X, Y=None, feature_weight=None, categorical_features=None):\n",
    "    \"\"\"Computes the gower distances between X and Y\n",
    "\n",
    "    Gower is a similarity measure for categorical, boolean and numerical mixed\n",
    "    data.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, or pandas.DataFrame, shape (n_samples, n_features)\n",
    "\n",
    "    Y : array-like, or pandas.DataFrame, shape (n_samples, n_features)\n",
    "\n",
    "    feature_weight :  array-like, shape (n_features)\n",
    "        According the Gower formula, feature_weight is an attribute weight.\n",
    "\n",
    "    categorical_features: array-like, shape (n_features)\n",
    "        Indicates with True/False whether a column is a categorical attribute.\n",
    "        This is useful when categorical atributes are represented as integer\n",
    "        values. Categorical ordinal attributes are treated as numeric, and must\n",
    "        be marked as false.\n",
    "        \n",
    "        Alternatively, the categorical_features array can be represented only\n",
    "        with the numerical indexes of the categorical attribtes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    similarities : ndarray, shape (n_samples, n_samples)\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    The non-numeric features, and numeric feature ranges are determined from X and not Y.\n",
    "    No support for sparse matrices.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if issparse(X) or issparse(Y):\n",
    "        raise TypeError(\"Sparse matrices are not supported for gower distance\")\n",
    "        \n",
    "    y_none = Y is None\n",
    "    \n",
    "    \n",
    "    # It is necessary to convert to ndarray in advance to define the dtype\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "    array_type = np.object\n",
    "    # this is necessary as strangelly the validator is rejecting numeric\n",
    "    # arrays with NaN\n",
    "    if  np.issubdtype(X.dtype, np.number) and (np.isfinite(X.sum()) or np.isfinite(X).all()):\n",
    "        array_type = type(np.zeros(1,X.dtype).flat[0])\n",
    "    \n",
    "    X, Y = check_pairwise_arrays(X, Y, precomputed=False, dtype=array_type)\n",
    "    \n",
    "    n_rows, n_cols = X.shape\n",
    "    \n",
    "    if categorical_features is None:\n",
    "        categorical_features = np.zeros(n_cols, dtype=bool)\n",
    "        for col in range(n_cols):\n",
    "            # In numerical columns, None is converted to NaN,\n",
    "            # and the type of NaN is recognized as a number subtype\n",
    "            if not np.issubdtype(type(X[0, col]), np.number):\n",
    "                categorical_features[col]=True\n",
    "    else:          \n",
    "        categorical_features = np.array(categorical_features)\n",
    "    \n",
    "    \n",
    "    #if categorical_features.dtype == np.int32:\n",
    "    if np.issubdtype(categorical_features.dtype, np.int):\n",
    "        new_categorical_features = np.zeros(n_cols, dtype=bool)\n",
    "        new_categorical_features[categorical_features] = True\n",
    "        categorical_features = new_categorical_features\n",
    "    \n",
    "    #print(categorical_features)\n",
    "  \n",
    "    # Categorical columns\n",
    "    X_cat =  X[:,categorical_features]\n",
    "    \n",
    "    # Numerical columns\n",
    "    X_num = X[:,np.logical_not(categorical_features)]\n",
    "    ranges_of_numeric = None\n",
    "    max_of_numeric = None\n",
    "    \n",
    "        \n",
    "    # Calculates the normalized ranges and max values of numeric values\n",
    "    _ ,num_cols=X_num.shape\n",
    "    ranges_of_numeric = np.zeros(num_cols)\n",
    "    max_of_numeric = np.zeros(num_cols)\n",
    "    for col in range(num_cols):\n",
    "        col_array = X_num[:, col].astype(np.float32) \n",
    "        max = np.nanmax(col_array)\n",
    "        min = np.nanmin(col_array)\n",
    "     \n",
    "        if np.isnan(max):\n",
    "            max = 0.0\n",
    "        if np.isnan(min):\n",
    "            min = 0.0\n",
    "        max_of_numeric[col] = max\n",
    "        ranges_of_numeric[col] = (1 - min / max) if (max != 0) else 0.0\n",
    "\n",
    "\n",
    "    # This is to normalize the numeric values between 0 and 1.\n",
    "    X_num = np.divide(X_num ,max_of_numeric,out=np.zeros_like(X_num), where=max_of_numeric!=0)\n",
    "\n",
    "    \n",
    "    if feature_weight is None:\n",
    "        feature_weight = np.ones(n_cols)\n",
    "        \n",
    "    feature_weight_cat=feature_weight[categorical_features]\n",
    "    feature_weight_num=feature_weight[np.logical_not(categorical_features)]\n",
    "    \n",
    "    \n",
    "    y_n_rows, _ = Y.shape\n",
    "    \n",
    "    dm = np.zeros((n_rows, y_n_rows), dtype=np.float32)\n",
    "        \n",
    "    feature_weight_sum = feature_weight.sum()\n",
    "\n",
    "    Y_cat=None\n",
    "    Y_num=None\n",
    "    \n",
    "    if not y_none:\n",
    "        Y_cat = Y[:,categorical_features]\n",
    "        Y_num = Y[:,np.logical_not(categorical_features)]\n",
    "        # This is to normalize the numeric values between 0 and 1.\n",
    "        Y_num = np.divide(Y_num ,max_of_numeric,out=np.zeros_like(Y_num), where=max_of_numeric!=0)\n",
    "    else:\n",
    "        Y_cat=X_cat\n",
    "        Y_num = X_num\n",
    "        \n",
    "    for i in range(n_rows):\n",
    "        j_start= i\n",
    "        \n",
    "        # for non square results\n",
    "        if n_rows != y_n_rows:\n",
    "            j_start = 0\n",
    "\n",
    "      \n",
    "        Y_cat[j_start:n_rows,:]\n",
    "        Y_num[j_start:n_rows,:]\n",
    "        result= _gower_distance_row(X_cat[i,:], X_num[i,:],Y_cat[j_start:n_rows,:],\n",
    "                                    Y_num[j_start:n_rows,:],feature_weight_cat,feature_weight_num,\n",
    "                                    feature_weight_sum,categorical_features,ranges_of_numeric,\n",
    "                                    max_of_numeric) \n",
    "        dm[i,j_start:]=result\n",
    "        dm[i:,j_start]=result\n",
    "        \n",
    "\n",
    "    return dm\n",
    "\n",
    "\n",
    "def _gower_distance_row(xi_cat,xi_num,xj_cat,xj_num,feature_weight_cat,feature_weight_num,\n",
    "                        feature_weight_sum,categorical_features,ranges_of_numeric,max_of_numeric ):\n",
    "    # categorical columns\n",
    "    sij_cat = np.where(xi_cat == xj_cat,np.zeros_like(xi_cat),np.ones_like(xi_cat))\n",
    "    sum_cat = np.multiply(feature_weight_cat,sij_cat).sum(axis=1) \n",
    "\n",
    "    # numerical columns\n",
    "    abs_delta=np.absolute( xi_num-xj_num)\n",
    "    sij_num=np.divide(abs_delta, ranges_of_numeric, out=np.zeros_like(abs_delta), where=ranges_of_numeric!=0)\n",
    "\n",
    "    sum_num = np.multiply(feature_weight_num,sij_num).sum(axis=1)\n",
    "    sums= np.add(sum_cat,sum_num)\n",
    "    sum_sij = np.divide(sums,feature_weight_sum)\n",
    "    return sum_sij\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A.  (15 points) Using the Gower distances in the matrix D computed by the provided function, find the most and least similar pairs of colleges in the dataset.  \n",
    "\n",
    "Note that an item is most similar to itself (distance = 0.) but you need to disallow this case since we actually care about finding two *distinct* items not along the diagonal that are most similar. One quick way to accomplish this is to replace the zeros along the diagonal of the distance matrix D returned by the gower_distances function, with a very large number (e.g. 1000) that wouldn't occur as a distance in practice.\n",
    "\n",
    "You may also find numpy's `unravel_index` function, in combination with `argmax` or `argmin`, useful for finding min/max elements in an array. Remember that the least similar elements will have maximum distance from each other, and most similar will have minimum distance.\n",
    "\n",
    "Your function should return a 2-element tuple, consisting itself of two tuples: the first tuple should be the names (via the College.Name field) of the two colleges that are *least* similar according the Gower distance. The second tuple should name the *most* similar colleges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = gower_distances(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(777, 776)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = D[~np.eye(D.shape[0],dtype=bool)].reshape(D.shape[0],-1)\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([100, 581]), array([580, 100]))\n",
      "(array([ 29, 260]), array([259,  29]))\n"
     ]
    }
   ],
   "source": [
    "lowest = np.where(D == np.min(D))\n",
    "highest = np.where((D == np.max(D)))\n",
    "print(highest)\n",
    "print(lowest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 581]\n",
      "[ 29 260]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('Center for Creative Studies', 'Texas A&M Univ. at College Station'),\n",
       " ('Augustana College IL', 'Hope College'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest = np.where(D == np.min(D))[0]\n",
    "highest = np.where((D == np.max(D)))[0]\n",
    "print(highest)\n",
    "print(lowest)\n",
    "lowest_pair = (df['College.Name'].iloc[lowest[0]], df['College.Name'].iloc[lowest[1]])\n",
    "highest_pair = (df['College.Name'].iloc[highest[0]], df['College.Name'].iloc[highest[1]])\n",
    "result = (highest_pair, lowest_pair)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100, 581])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b4705b6ebf62f3f0eca6d2812359ec59",
     "grade": false,
     "grade_id": "cell-06feac2706761a09",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Center for Creative Studies', 'Texas A&M Univ. at College Station'),\n",
       " ('Augustana College IL', 'Hope College'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "def answer_mixedfeatures_a():\n",
    "    result = (highest_pair, lowest_pair)\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return result \n",
    "answer_mixedfeatures_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bf73a4ad29ac7b73896a4f24436b59cd",
     "grade": true,
     "grade_id": "cell-c2e6dd563f89e13c",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_mixedfeatures_a()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q4a: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 2, \"Q4a: Your return tuple should have 2 values. \"\n",
    "\n",
    "assert isinstance(stu_ans[0], tuple), \"Q4a: The first element should be a tuple. \"\n",
    "assert len(stu_ans[0]) == 2, \"Q4a: The first tuple should have 2 values. \"\n",
    "\n",
    "assert isinstance(stu_ans[1], tuple), \"Q4a: The second element should be a tuple. \"\n",
    "assert len(stu_ans[1]) == 2, \"Q4a: The second tuple should have 2 values. \"\n",
    "\n",
    "assert isinstance(stu_ans[0][0], str), \"Q4a: Element [0][0] must be a string.\"\n",
    "assert isinstance(stu_ans[0][1], str), \"Q4a: Element [0][0] must be a string.\"\n",
    "assert isinstance(stu_ans[1][0], str), \"Q4a: Element [1][0] must be a string.\"\n",
    "assert isinstance(stu_ans[1][1], str), \"Q4a: Element [1][1] must be a string.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B. (10 points) With the dissimilarity matrix you computed in Part A, now run k-medoids clustering with 3 clusters, and return the central representative found by k-medoids for each cluster.\n",
    "\n",
    "Your function should return a dataframe with 3 rows corresponding to the k-medoids representatives for each cluster, and have columns named \"College.Name\", \"Private\", \"Enroll\",\"Expend\", and \"Grad.Rate\" containing those values of the representative college.\n",
    "\n",
    "NOTE: Call KMedoids using the metric = 'precomputed' option, and random_state = 42. Make sure your dataframe is ordered in the same order as the medoid indices that are returned by kmedoids.medoid_indices_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = gower_distances(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "kmedoids = KMedoids(n_clusters=3, metric = 'precomputed', random_state= 42).fit(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10, 748, 654])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmedoids.medoid_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>College.Name</th>\n",
       "      <th>Private</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alfred University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>472</td>\n",
       "      <td>10932</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>Westminster College MO</td>\n",
       "      <td>Yes</td>\n",
       "      <td>184</td>\n",
       "      <td>7925</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>University of North Carolina at Wilmington</td>\n",
       "      <td>No</td>\n",
       "      <td>1449</td>\n",
       "      <td>6005</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   College.Name Private  Enroll  Expend  \\\n",
       "10                            Alfred University     Yes     472   10932   \n",
       "748                      Westminster College MO     Yes     184    7925   \n",
       "654  University of North Carolina at Wilmington      No    1449    6005   \n",
       "\n",
       "     Grad.Rate  \n",
       "10          73  \n",
       "748         62  \n",
       "654         55  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['College.Name'].iloc[highest[0]], df['College.Name'].iloc[highest[1]])\n",
    "\n",
    "new_df = df[[\"College.Name\", \"Private\", \"Enroll\",\"Expend\", \"Grad.Rate\"]]\n",
    "#new_df.head()\n",
    "centroid_df = new_df.iloc[[10, 748, 654]]\n",
    "centroid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "baecc29f3ca0ef9540619d4ccf4dad04",
     "grade": false,
     "grade_id": "cell-6adf33ccf58ba453",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>College.Name</th>\n",
       "      <th>Private</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alfred University</td>\n",
       "      <td>Yes</td>\n",
       "      <td>472</td>\n",
       "      <td>10932</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>Westminster College MO</td>\n",
       "      <td>Yes</td>\n",
       "      <td>184</td>\n",
       "      <td>7925</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>University of North Carolina at Wilmington</td>\n",
       "      <td>No</td>\n",
       "      <td>1449</td>\n",
       "      <td>6005</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   College.Name Private  Enroll  Expend  \\\n",
       "10                            Alfred University     Yes     472   10932   \n",
       "748                      Westminster College MO     Yes     184    7925   \n",
       "654  University of North Carolina at Wilmington      No    1449    6005   \n",
       "\n",
       "     Grad.Rate  \n",
       "10          73  \n",
       "748         62  \n",
       "654         55  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "def answer_mixedfeatures_b():\n",
    "    #result = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return centroid_df\n",
    "\n",
    "answer_mixedfeatures_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "73ec3b0bf48f9b0c2f667b3a654c2991",
     "grade": true,
     "grade_id": "cell-3d425130ecc2d2d3",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_mixedfeatures_b()\n",
    "\n",
    "assert isinstance(stu_ans, pd.DataFrame), \"Q4b: Your function should return a pandas DataFrame. \"\n",
    "assert stu_ans.shape == (3, 5), \"Q4b: The shape of your dataframe isn't correct. \"\n",
    "\n",
    "assert list(stu_ans.columns) == ['College.Name', 'Private', 'Enroll', 'Expend', 'Grad.Rate'], \"Q4b: Please check the column names of your DataFrame.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_unsupervised_learning_v1_assignment2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
